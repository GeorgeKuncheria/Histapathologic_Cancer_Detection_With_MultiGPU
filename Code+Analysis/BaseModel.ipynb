{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd8b63f5-461b-4264-aca8-70d4cf2948ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d615c8-0b1e-40ed-bc0e-d094b3c4a3da",
   "metadata": {},
   "source": [
    "# Preparing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9dfaec-7d44-418f-9b1a-ba9dfc292f33",
   "metadata": {},
   "source": [
    "## 1. Load the Balanced CSV & creating a Path Map \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a58a6b7-0a7f-4de7-b2c3-628e5fd2fab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping image paths...\n",
      "Mapped 240705 image paths.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('oral_cancer_balanced.csv')\n",
    "\n",
    "print(\"Mapping image paths...\")\n",
    "path_map = {}\n",
    "\n",
    "for root, dirs, files in os.walk('Data'):\n",
    "    if 'val' in dirs:\n",
    "        dirs.remove('val')\n",
    "        \n",
    "    for file in files:\n",
    "        if file.lower().endswith(('.jpg', '.jpeg')):\n",
    "            # Optional: Extra safety check ensuring 'val' isn't in the path string\n",
    "            if 'val' not in root:\n",
    "                path_map[file] = os.path.join(root, file)\n",
    "\n",
    "print(f\"Mapped {len(path_map)} image paths.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ad94d1-a7e8-41b0-8972-777b486dbfe1",
   "metadata": {},
   "source": [
    "## 2. Defining the Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70d29f58-b787-42a1-b2a1-c91f96f68e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OralCancerDataset(Dataset):\n",
    "    def __init__(self, dataframe, path_map, transform=None):\n",
    "        '''\n",
    "        dataframe: Pandas dataframe containing 'id' and 'label'\n",
    "        path_map: Dictionary mapping filenames to full file paths\n",
    "        transform: PyTorch transforms (augmentation/normalization)\n",
    "        '''\n",
    "        self.data = dataframe\n",
    "        self.path_map = path_map\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get ID and Label from the dataframe\n",
    "        img_id = self.data.iloc[idx]['id']\n",
    "        label = int(self.data.iloc[idx]['label'])\n",
    "        \n",
    "        # Find the full path using our map\n",
    "        # (We use .get() to handle cases where a file might be missing safely)\n",
    "        img_path = self.path_map.get(img_id)\n",
    "        \n",
    "        if img_path is None:\n",
    "            raise FileNotFoundError(f\"Image {img_id} not found in Data folders.\")\n",
    "\n",
    "        # Open Image\n",
    "        image = Image.open(img_path).convert('RGB') # Ensure it's RGB\n",
    "\n",
    "        # Apply Transforms\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd0d27c-0e5c-4b01-817c-c878f8e67e04",
   "metadata": {},
   "source": [
    "## 3. Defining Transforms (Augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b90acbc9-ebfe-44b3-a8db-99e6b55694ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image settings\n",
    "IMG_SIZE = 96\n",
    "\n",
    "# 1. Training Transforms (With Augmentation)\n",
    "# We add noise/flips to make the model generalize better\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# 2. Validation Transforms (Clean)\n",
    "# No flips or rotations; just resize and normalize\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ba3bfc-599f-4f66-aedd-b6a70dbb5b96",
   "metadata": {},
   "source": [
    "## 4. Spliting Data and Initializing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90bff1e2-88ce-4132-b947-9860f178507e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets successfully created.\n",
      "------------------------------\n",
      "Train data size:      192452 images\n",
      "Validation data size: 48114 images\n"
     ]
    }
   ],
   "source": [
    "# Split the Dataframe (80% Train, 20% Validation)\n",
    "train_df, val_df = train_test_split(\n",
    "    df, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=df['label'] # Ensures equal balance of 0s and 1s in both sets\n",
    ")\n",
    "\n",
    "#  Create the Datasets\n",
    "# We pass 'train_transforms' to the train set and 'val_transforms' to the val set\n",
    "train_dataset = OralCancerDataset(train_df, path_map, transform=train_transforms)\n",
    "val_dataset = OralCancerDataset(val_df, path_map, transform=val_transforms)\n",
    "\n",
    "print(\"Datasets successfully created.\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Train data size:      {len(train_dataset)} images\")\n",
    "print(f\"Validation data size: {len(val_dataset)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91962e3-3233-4668-94d1-db9468f4e150",
   "metadata": {},
   "source": [
    "## 5. Creating DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6eb978f-13dd-4c0a-bc6a-ef5b9d44addf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoaders ready with Batch Size 64.\n",
      "Training Batches: 3008\n",
      "Validation Batches: 752\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "# Create DataLoaders\n",
    "# shuffle=True for training to mix batches\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "\n",
    "# shuffle=False for validation (order doesn't matter, but usually kept static)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"DataLoaders ready with Batch Size {BATCH_SIZE}.\")\n",
    "print(f\"Training Batches: {len(train_loader)}\")\n",
    "print(f\"Validation Batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "017bbb16-2870-4625-9716-fc0f25b4c031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batch images: torch.Size([64, 3, 96, 96]), labels: torch.Size([64])\n",
      "Validation batch images: torch.Size([64, 3, 96, 96]), labels: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "images, labels = next(iter(train_loader))\n",
    "print(f\"Train batch images: {images.shape}, labels: {labels.shape}\")\n",
    "\n",
    "images, labels = next(iter(val_loader))\n",
    "print(f\"Validation batch images: {images.shape}, labels: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccd725d-7c02-4685-b73d-98f1dd4daa8f",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb527929-eb59-4339-b2da-2e5c1676ab05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3cba9dfe-87b2-4f85-bdf7-96572794ba35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        \n",
    "        # --- Block 1 ---\n",
    "        # Input: (Batch_Size, 3, 96, 96)\n",
    "        # Conv: Maintains size (padding=1) -> (32, 96, 96)\n",
    "        # Pool: Halves size -> (32, 48, 48)\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        # --- Block 2 ---\n",
    "        # Input: (32, 48, 48)\n",
    "        # Conv: Maintains size -> (64, 48, 48)\n",
    "        # Pool: Halves size -> (64, 24, 24)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # --- Block 3 ---\n",
    "        # Input: (64, 24, 24)\n",
    "        # Conv: Maintains size -> (128, 24, 24)\n",
    "        # Pool: Halves size -> (128, 12, 12)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # Max Pooling Layer (used in all blocks)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # --- Classifier ---\n",
    "        # Flattened Input size calculation:\n",
    "        # Depth (128) * Height (12) * Width (12) = 18,432 features\n",
    "        self.fc1 = nn.Linear(128 * 12 * 12, 256)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(256, 1)  # Output: 1 Logit\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Block 1\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x) # Image is now 48x48\n",
    "        \n",
    "        # Block 2\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x) # Image is now 24x24\n",
    "        \n",
    "        # Block 3\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x) # Image is now 12x12\n",
    "        \n",
    "        # Flattening\n",
    "        # Flatten dimensions 1, 2, 3 into a single vector per image\n",
    "        x = x.view(-1, 128 * 12 * 12) \n",
    "        \n",
    "        # Fully Connected\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cffe717-7607-413d-870c-28d7d5dd6b71",
   "metadata": {},
   "source": [
    "## Dummy Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de48c019-939d-4072-b5f0-65f046f2336b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Architecture Created.\n",
      "Input Shape:  torch.Size([1, 3, 96, 96])\n",
      "Output Shape: torch.Size([1, 1]) (Should be [1, 1])\n",
      "Test Passed: Dimensions align correctly.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Creating the model\n",
    "model = SimpleCNN()\n",
    "\n",
    "# Create a dummy batch of 1 image with shape (1, 3, 96, 96)\n",
    "dummy_input = torch.randn(1, 3, 96, 96)\n",
    "\n",
    "# Pass it through the model\n",
    "output = model(dummy_input)\n",
    "\n",
    "print(f\"Model Architecture Created.\")\n",
    "print(f\"Input Shape:  {dummy_input.shape}\")\n",
    "print(f\"Output Shape: {output.shape} (Should be [1, 1])\")\n",
    "print(\"Test Passed: Dimensions align correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427ee47d-fb60-4a0c-ac94-7d94f90c027d",
   "metadata": {},
   "source": [
    "## Creating code diagnostic and assigning cuda to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba96063a-d5b6-4365-95a3-dc1d4c966ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple model architecture:\n",
      "GPU: cuda\n",
      "SimpleCNN(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=18432, out_features=256, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "simple_model = SimpleCNN().to(device)\n",
    "print('Simple model architecture:')\n",
    "print(f\"GPU: {device}\")\n",
    "print(simple_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a19fc4-0efd-4264-8ada-326f90c64301",
   "metadata": {},
   "source": [
    "## Creating Optimizer and Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9bc3ae25-371b-48af-b258-cdfd3aaa2402",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "# For a binary classifier, this function applies sigmoid while evaluating the Loss.\n",
    "loss_function_simple_model = nn.BCEWithLogitsLoss()\n",
    "\n",
    "\n",
    "\n",
    "# We use the Adam optimizer because it is a good optimization method.\n",
    "optimizer_simple_model = optim.Adam(simple_model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa541096-d957-4aab-9d6c-fedbc44f2186",
   "metadata": {},
   "source": [
    "## Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ccfc984-dd0a-4fe6-98c0-bfc7f68c6f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for adjusting the learning rate after each epoch, in order to obtain better results, \n",
    "# as it can happen that the parameters are arriving to an optimal value and the learning rate needs to be reduced.\n",
    "\n",
    "scheduler_simple_model = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer_simple_model, mode='max', factor=0.5, patience=2, min_lr=1e-5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534ab356-e585-41c2-8e37-24053d28663b",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e2fe6ba8-e667-4f18-b2c3-1b4f3eb10bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_model(num_epochs, optimizer, model, loss_function, scheduler, model_save_path):\n",
    "    \n",
    "    # Track the best accuracy to save the model\n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    print(f\"Starting training for {num_epochs} epochs...\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # --- TRAINING PHASE ---\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            # Move data to GPU\n",
    "            images = images.to(device)\n",
    "            # Reshape labels to (Batch_Size, 1) to match model output\n",
    "            labels = labels.float().unsqueeze(1).to(device)\n",
    "\n",
    "            # 1. Zero Gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 2. Forward Pass\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # 3. Calculate Loss\n",
    "            loss = loss_function(outputs, labels)\n",
    "            \n",
    "            # 4. Backward Pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # 5. Optimization Step\n",
    "            optimizer.step()\n",
    "\n",
    "            # --- Metrics Calculation ---\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            \n",
    "            # Calculate Training Accuracy\n",
    "            # Apply Sigmoid to convert logits to probabilities (0 to 1)\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            preds = probs >= 0.5\n",
    "            train_correct += (preds == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "        \n",
    "        # Calculate Epoch Averages\n",
    "        epoch_train_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_train_acc = train_correct / train_total\n",
    "        train_losses.append(epoch_train_loss)\n",
    "\n",
    "        # --- VALIDATION PHASE ---\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad(): # No gradients needed for validation\n",
    "            for images, labels in val_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.float().unsqueeze(1).to(device)\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = loss_function(outputs, labels)\n",
    "                \n",
    "                val_running_loss += loss.item() * images.size(0)\n",
    "                \n",
    "                # Validation Accuracy\n",
    "                probs = torch.sigmoid(outputs)\n",
    "                preds = probs >= 0.5\n",
    "                val_correct += (preds == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "        epoch_val_loss = val_running_loss / len(val_loader.dataset)\n",
    "        epoch_val_acc = val_correct / val_total\n",
    "        val_losses.append(epoch_val_loss)\n",
    "\n",
    "        # --- LOGGING ---\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "        print(f\"   Train Loss: {epoch_train_loss:.4f} | Train Acc: {epoch_train_acc:.4f}\")\n",
    "        print(f\"   Val Loss:   {epoch_val_loss:.4f} | Val Acc:   {epoch_val_acc:.4f}\")\n",
    "\n",
    "        # --- SCHEDULER & CHECKPOINTING ---\n",
    "        # Update Scheduler based on Validation Accuracy\n",
    "        scheduler.step(epoch_val_acc)\n",
    "        \n",
    "        # Save Model if Validation Accuracy Improves\n",
    "        if epoch_val_acc > best_val_acc:\n",
    "            print(f\"   --> Accuracy Improved ({best_val_acc:.4f} -> {epoch_val_acc:.4f}). Saving model...\")\n",
    "            best_val_acc = epoch_val_acc\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "        \n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "67e0beb3-54a6-4517-9b7b-7777a520a934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 10 epochs...\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch [1/10]\n",
      "   Train Loss: 0.5123 | Train Acc: 0.7748\n",
      "   Val Loss:   0.4011 | Val Acc:   0.8220\n",
      "   --> Accuracy Improved (0.0000 -> 0.8220). Saving model...\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch [2/10]\n",
      "   Train Loss: 0.4397 | Train Acc: 0.8056\n",
      "   Val Loss:   0.5499 | Val Acc:   0.7729\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch [3/10]\n",
      "   Train Loss: 0.4055 | Train Acc: 0.8219\n",
      "   Val Loss:   0.3564 | Val Acc:   0.8306\n",
      "   --> Accuracy Improved (0.8220 -> 0.8306). Saving model...\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch [4/10]\n",
      "   Train Loss: 0.3695 | Train Acc: 0.8434\n",
      "   Val Loss:   0.3018 | Val Acc:   0.8684\n",
      "   --> Accuracy Improved (0.8306 -> 0.8684). Saving model...\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch [5/10]\n",
      "   Train Loss: 0.3408 | Train Acc: 0.8568\n",
      "   Val Loss:   0.2696 | Val Acc:   0.8866\n",
      "   --> Accuracy Improved (0.8684 -> 0.8866). Saving model...\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch [6/10]\n",
      "   Train Loss: 0.3209 | Train Acc: 0.8671\n",
      "   Val Loss:   0.2589 | Val Acc:   0.8935\n",
      "   --> Accuracy Improved (0.8866 -> 0.8935). Saving model...\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch [7/10]\n",
      "   Train Loss: 0.3058 | Train Acc: 0.8739\n",
      "   Val Loss:   0.2397 | Val Acc:   0.9030\n",
      "   --> Accuracy Improved (0.8935 -> 0.9030). Saving model...\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch [8/10]\n",
      "   Train Loss: 0.2951 | Train Acc: 0.8798\n",
      "   Val Loss:   0.2593 | Val Acc:   0.8958\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch [9/10]\n",
      "   Train Loss: 0.2861 | Train Acc: 0.8828\n",
      "   Val Loss:   0.2773 | Val Acc:   0.8863\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch [10/10]\n",
      "   Train Loss: 0.2796 | Train Acc: 0.8870\n",
      "   Val Loss:   0.2380 | Val Acc:   0.8974\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Training Complete.\n"
     ]
    }
   ],
   "source": [
    "save_path = 'simple_cnn_best.pth'\n",
    "\n",
    "# Run Training\n",
    "history = training_model(\n",
    "    num_epochs=10, \n",
    "    optimizer=optimizer_simple_model, \n",
    "    model=simple_model, \n",
    "    loss_function=loss_function_simple_model, \n",
    "    scheduler=scheduler_simple_model, \n",
    "    model_save_path=save_path\n",
    ")\n",
    "\n",
    "print(\"\\nTraining Complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (hpc)",
   "language": "python",
   "name": "hpc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
