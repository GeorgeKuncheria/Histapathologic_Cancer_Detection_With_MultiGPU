{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77303ccc-01e4-49f4-be8b-4daf32850ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ddp_benchmark.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ddp_benchmark.py\n",
    "import os\n",
    "import time\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.multiprocessing as mp\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import socket\n",
    "import warnings\n",
    "\n",
    "# --- WARNING SUPPRESSION ---\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "# ==========================================\n",
    "# 1. SETUP\n",
    "# ==========================================\n",
    "\n",
    "def find_free_port():\n",
    "    \"\"\"Finds a random open port on the machine.\"\"\"\n",
    "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "        s.bind(('', 0)) # Bind to port 0 lets the OS pick an available port\n",
    "        return s.getsockname()[1]\n",
    "\n",
    "def setup(rank, world_size, port):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = str(port)\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "def get_gpu_utilization(device_id):\n",
    "    try:\n",
    "        result = subprocess.check_output(\n",
    "            ['nvidia-smi', f'--id={device_id}', '--query-gpu=utilization.gpu', '--format=csv,noheader,nounits'],\n",
    "            encoding='utf-8'\n",
    "        )\n",
    "        return float(result.strip())\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "# ==========================================\n",
    "# 2. DATA & MODEL\n",
    "# ==========================================\n",
    "\n",
    "class OralCancerDataset(Dataset):\n",
    "    def __init__(self, dataframe, path_map, transform=None):\n",
    "        self.data = dataframe\n",
    "        self.path_map = path_map\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.data.iloc[idx]['id']\n",
    "        label = int(self.data.iloc[idx]['label'])\n",
    "        \n",
    "        img_path = self.path_map.get(img_id)\n",
    "        if img_path is None:\n",
    "            image = Image.new('RGB', (96, 96)) \n",
    "        else:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(128 * 12 * 12, 256)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = x.view(-1, 128 * 12 * 12)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# ==========================================\n",
    "# 3. UNIVERSAL TRAINER (With Checkpointing)\n",
    "# ==========================================\n",
    "\n",
    "def ddp_train_process(rank, world_size, df, path_map, batch_size, num_epochs, use_amp, shared_list, port):\n",
    "    setup(rank, world_size, port)\n",
    "    \n",
    "    # --- Checkpoint Directory Setup ---\n",
    "    ckpt_dir = \"checkpoints_for_ddp\"\n",
    "    if rank == 0:\n",
    "        os.makedirs(ckpt_dir, exist_ok=True)\n",
    "    \n",
    "    # Create a unique filename for this specific configuration\n",
    "    # e.g., ckpt_gpu2_batch128_amp.pth\n",
    "    precision_tag = \"amp\" if use_amp else \"fp32\"\n",
    "    ckpt_filename = f\"ckpt_gpu{world_size}_batch{batch_size}_{precision_tag}.pth\"\n",
    "    ckpt_path = os.path.join(ckpt_dir, ckpt_filename)\n",
    "    \n",
    "    # --- Hardware Info ---\n",
    "    gpu_name = torch.cuda.get_device_name(rank)\n",
    "    gpu_props = torch.cuda.get_device_properties(rank)\n",
    "    total_mem_mb = gpu_props.total_memory / (1024**2)\n",
    "    precision_str = \"AMP\" if use_amp else \"FP32\"\n",
    "    \n",
    "    # --- Data ---\n",
    "    IMG_SIZE = 96\n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "    ])\n",
    "    val_transforms = transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "    ])\n",
    "    \n",
    "    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n",
    "    train_dataset = OralCancerDataset(train_df, path_map, transform=train_transforms)\n",
    "    val_dataset = OralCancerDataset(val_df, path_map, transform=val_transforms)\n",
    "    \n",
    "    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank)\n",
    "    val_sampler = DistributedSampler(val_dataset, num_replicas=world_size, rank=rank, shuffle=False)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=0, sampler=train_sampler)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, sampler=val_sampler)\n",
    "\n",
    "    # --- Model & Optimizer ---\n",
    "    model = SimpleCNN().to(rank)\n",
    "    model = DDP(model, device_ids=[rank])\n",
    "    loss_function = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    scaler = torch.cuda.amp.GradScaler() if use_amp else None\n",
    "    \n",
    "    # --- CHECKPOINT LOADING LOGIC ---\n",
    "    start_epoch = 0\n",
    "    if os.path.exists(ckpt_path):\n",
    "        # We load to the specific GPU device (rank)\n",
    "        map_location = f\"cuda:{rank}\"\n",
    "        checkpoint = torch.load(ckpt_path, map_location=map_location)\n",
    "        \n",
    "        model.module.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        if use_amp and checkpoint.get('scaler_state_dict'):\n",
    "            scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
    "            \n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        \n",
    "        if rank == 0:\n",
    "            print(f\"   [Rank 0] Found checkpoint {ckpt_filename}. Resuming from Epoch {start_epoch+1}\")\n",
    "    \n",
    "    # Ensure all ranks wait for the load to finish\n",
    "    dist.barrier()\n",
    "\n",
    "    if start_epoch < num_epochs:\n",
    "        if rank == 0:\n",
    "            print(f\"   [Rank {rank}] Ready | {precision_str} | {gpu_name}\")\n",
    "    else:\n",
    "        if rank == 0:\n",
    "            print(f\"   [Rank {rank}] Experiment finished in previous run. Skipping.\")\n",
    "\n",
    "    # --- Training Loop ---\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        train_sampler.set_epoch(epoch)\n",
    "        torch.cuda.reset_peak_memory_stats(rank)\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(rank)\n",
    "            labels = labels.float().unsqueeze(1).to(rank)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if use_amp:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(images)\n",
    "                    loss = loss_function(outputs, labels)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                outputs = model(images)\n",
    "                loss = loss_function(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * images.size(0)\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            preds = probs >= 0.5\n",
    "            train_correct += (preds == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "            \n",
    "        epoch_time = time.time() - start_time\n",
    "        \n",
    "        # Stats\n",
    "        max_mem_mb = torch.cuda.max_memory_allocated(rank) / (1024**2)\n",
    "        mem_util_percent = (max_mem_mb / total_mem_mb) * 100\n",
    "        gpu_util_percent = get_gpu_utilization(rank)\n",
    "        util_str = f\"GPU:{gpu_util_percent:.1f}% / Mem:{mem_util_percent:.1f}%\"\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images = images.to(rank)\n",
    "                labels = labels.float().unsqueeze(1).to(rank)\n",
    "                \n",
    "                if use_amp:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        outputs = model(images)\n",
    "                        loss = loss_function(outputs, labels)\n",
    "                else:\n",
    "                    outputs = model(images)\n",
    "                    loss = loss_function(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item() * images.size(0)\n",
    "                probs = torch.sigmoid(outputs)\n",
    "                preds = probs >= 0.5\n",
    "                val_correct += (preds == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "        # Sync Metrics\n",
    "        metrics_tensor = torch.tensor([train_loss, train_correct, train_total, val_loss, val_correct, val_total], device=rank)\n",
    "        dist.all_reduce(metrics_tensor, op=dist.ReduceOp.SUM)\n",
    "        \n",
    "        global_train_loss = metrics_tensor[0].item() / metrics_tensor[2].item()\n",
    "        global_train_acc  = metrics_tensor[1].item() / metrics_tensor[2].item()\n",
    "        global_val_loss   = metrics_tensor[3].item() / metrics_tensor[5].item()\n",
    "        global_val_acc    = metrics_tensor[4].item() / metrics_tensor[5].item()\n",
    "        total_samples     = metrics_tensor[2].item()\n",
    "        train_throughput = total_samples / epoch_time\n",
    "\n",
    "        if rank == 0:\n",
    "            print(f\"   >>> [GLOBAL] {precision_str} | Epoch {epoch+1}: Acc={global_train_acc:.4f} | Throughput={train_throughput:.1f} img/s\")\n",
    "            \n",
    "            # --- SAVE CHECKPOINT ---\n",
    "            print(f\"   ... Saving checkpoint to {ckpt_filename} ...\")\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.module.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scaler_state_dict': scaler.state_dict() if use_amp else None,\n",
    "            }, ckpt_path)\n",
    "            \n",
    "            # --- LOG TO LIST ---\n",
    "            shared_list.append({\n",
    "                'precision': 'AMP' if use_amp else 'FP32', \n",
    "                'gpu_count': world_size,\n",
    "                'batch_size': batch_size,\n",
    "                'epoch': epoch + 1,\n",
    "                'train_acc': global_train_acc,\n",
    "                'val_acc': global_val_acc,\n",
    "                'train_loss': global_train_loss,\n",
    "                'val_loss': global_val_loss,\n",
    "                'train_throughput': train_throughput,\n",
    "                'epoch_time': epoch_time,\n",
    "                'gpu_util/mem_util': util_str,\n",
    "                'gpu_name': gpu_name,\n",
    "                'each_gpu_memory': f\"{total_mem_mb:.0f}MB\"\n",
    "            })\n",
    "            \n",
    "            # --- INCREMENTAL CSV BACKUP (Optional but good) ---\n",
    "            # This appends to a file so you don't lose data if the full script crashes later\n",
    "            backup_file = \"benchmark_master_results_for_ddp_backup.csv\"\n",
    "            header = not os.path.exists(backup_file)\n",
    "            temp_df = pd.DataFrame([shared_list[-1]])\n",
    "            temp_df.to_csv(backup_file, mode='a', header=header, index=False)\n",
    "\n",
    "    cleanup()\n",
    "\n",
    "# ==========================================\n",
    "# 4. MAIN LOOP\n",
    "# ==========================================\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"Loading Data...\")\n",
    "    df = pd.read_csv('oral_cancer_balanced.csv')\n",
    "    \n",
    "    path_map = {}\n",
    "    for root, dirs, files in os.walk('Data'):\n",
    "        if 'val' in dirs: dirs.remove('val')\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.jpg', '.jpeg')) and 'val' not in root:\n",
    "                path_map[file] = os.path.join(root, file)\n",
    "\n",
    "    max_gpus = torch.cuda.device_count()\n",
    "    \n",
    "    target_gpu_counts = [1, 2,4]\n",
    "    gpu_counts = [g for g in target_gpu_counts if g <= max_gpus]\n",
    "    batch_sizes = [64, 128, 512]\n",
    "    amp_settings = [False]\n",
    "    epochs_to_test = 10\n",
    "    \n",
    "    manager = mp.Manager()\n",
    "    shared_metrics = manager.list()\n",
    "    \n",
    "    print(f\"Available GPUs: {max_gpus}\")\n",
    "    print(\"Starting Combined Benchmark Loop with Checkpointing...\")\n",
    "    \n",
    "    for n_gpus in gpu_counts:\n",
    "        for b_size in batch_sizes:\n",
    "            for use_amp in amp_settings:\n",
    "                \n",
    "                # --- AUTO-FIND FREE PORT ---\n",
    "                current_port = find_free_port()\n",
    "                \n",
    "                mode_name = \"DDP+AMP\" if use_amp else \"DDP (Standard)\"\n",
    "                print(f\"\\n--- Running: {n_gpus} GPU(s) | Batch {b_size} | {mode_name} | Port {current_port} ---\")\n",
    "                \n",
    "                try:\n",
    "                    mp.spawn(\n",
    "                        ddp_train_process,\n",
    "                        args=(n_gpus, df, path_map, b_size, epochs_to_test, use_amp, shared_metrics, current_port),\n",
    "                        nprocs=n_gpus,\n",
    "                        join=True\n",
    "                    )\n",
    "                    time.sleep(3) \n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error: {e}\")\n",
    "\n",
    "    print(\"\\nSaving Master CSV...\")\n",
    "    if len(shared_metrics) > 0:\n",
    "        results_df = pd.DataFrame(list(shared_metrics))\n",
    "        cols = [\n",
    "            'precision', 'gpu_count', 'batch_size', 'epoch', \n",
    "            'train_acc', 'val_acc', 'train_loss', 'val_loss', \n",
    "            'train_throughput', 'epoch_time', 'gpu_util/mem_util',\n",
    "            'gpu_name', 'each_gpu_memory'\n",
    "        ]\n",
    "        results_df = results_df[cols]\n",
    "        results_df.to_csv('benchmark_master_results_for_ddp.csv', index=False)\n",
    "        print(\"Saved to 'benchmark_master_results_for_ddp.csv'\")\n",
    "        print(results_df.head(10))\n",
    "    else:\n",
    "        print(\"No results collected due to errors. Check 'benchmark_master_results_ddp_backup.csv' for partial data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98b25b68-7d64-4e3a-a787-6c250c6316ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data...\n",
      "Available GPUs: 4\n",
      "Starting Combined Benchmark Loop with Checkpointing...\n",
      "\n",
      "--- Running: 1 GPU(s) | Batch 64 | DDP (Standard) | Port 40137 ---\n",
      "   [Rank 0] Found checkpoint ckpt_gpu1_batch64_fp32.pth. Resuming from Epoch 11\n",
      "   [Rank 0] Experiment finished in previous run. Skipping.\n",
      "\n",
      "--- Running: 1 GPU(s) | Batch 128 | DDP (Standard) | Port 34735 ---\n",
      "   [Rank 0] Found checkpoint ckpt_gpu1_batch128_fp32.pth. Resuming from Epoch 11\n",
      "   [Rank 0] Experiment finished in previous run. Skipping.\n",
      "\n",
      "--- Running: 1 GPU(s) | Batch 512 | DDP (Standard) | Port 55445 ---\n",
      "   [Rank 0] Found checkpoint ckpt_gpu1_batch512_fp32.pth. Resuming from Epoch 2\n",
      "   [Rank 0] Ready | FP32 | Tesla P100-PCIE-12GB\n",
      "   >>> [GLOBAL] FP32 | Epoch 2: Acc=0.8438 | Throughput=556.8 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu1_batch512_fp32.pth ...\n",
      "   >>> [GLOBAL] FP32 | Epoch 3: Acc=0.8605 | Throughput=572.6 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu1_batch512_fp32.pth ...\n",
      "   >>> [GLOBAL] FP32 | Epoch 4: Acc=0.8711 | Throughput=567.1 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu1_batch512_fp32.pth ...\n",
      "   >>> [GLOBAL] FP32 | Epoch 5: Acc=0.8810 | Throughput=564.9 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu1_batch512_fp32.pth ...\n",
      "   >>> [GLOBAL] FP32 | Epoch 6: Acc=0.8877 | Throughput=570.1 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu1_batch512_fp32.pth ...\n",
      "   >>> [GLOBAL] FP32 | Epoch 7: Acc=0.8958 | Throughput=567.2 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu1_batch512_fp32.pth ...\n",
      "   >>> [GLOBAL] FP32 | Epoch 8: Acc=0.8990 | Throughput=573.6 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu1_batch512_fp32.pth ...\n",
      "   >>> [GLOBAL] FP32 | Epoch 9: Acc=0.9047 | Throughput=610.3 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu1_batch512_fp32.pth ...\n",
      "   >>> [GLOBAL] FP32 | Epoch 10: Acc=0.9086 | Throughput=587.5 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu1_batch512_fp32.pth ...\n",
      "\n",
      "--- Running: 2 GPU(s) | Batch 64 | DDP (Standard) | Port 49069 ---\n",
      "   [Rank 0] Ready | FP32 | Tesla P100-PCIE-12GB\n",
      "   >>> [GLOBAL] FP32 | Epoch 1: Acc=0.8185 | Throughput=1145.7 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu2_batch64_fp32.pth ...\n",
      "   >>> [GLOBAL] FP32 | Epoch 2: Acc=0.8510 | Throughput=1149.0 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu2_batch64_fp32.pth ...\n",
      "   >>> [GLOBAL] FP32 | Epoch 3: Acc=0.8675 | Throughput=1170.9 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu2_batch64_fp32.pth ...\n",
      "   >>> [GLOBAL] FP32 | Epoch 4: Acc=0.8828 | Throughput=1175.8 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu2_batch64_fp32.pth ...\n",
      "   >>> [GLOBAL] FP32 | Epoch 5: Acc=0.8948 | Throughput=1198.9 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu2_batch64_fp32.pth ...\n",
      "   >>> [GLOBAL] FP32 | Epoch 6: Acc=0.9039 | Throughput=1156.2 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu2_batch64_fp32.pth ...\n",
      "   >>> [GLOBAL] FP32 | Epoch 7: Acc=0.9109 | Throughput=1147.5 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu2_batch64_fp32.pth ...\n",
      "   >>> [GLOBAL] FP32 | Epoch 8: Acc=0.9156 | Throughput=1158.8 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu2_batch64_fp32.pth ...\n",
      "   >>> [GLOBAL] FP32 | Epoch 9: Acc=0.9211 | Throughput=1160.3 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu2_batch64_fp32.pth ...\n",
      "   >>> [GLOBAL] FP32 | Epoch 10: Acc=0.9264 | Throughput=1191.3 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu2_batch64_fp32.pth ...\n",
      "\n",
      "--- Running: 2 GPU(s) | Batch 128 | DDP (Standard) | Port 46573 ---\n",
      "   [Rank 0] Ready | FP32 | Tesla P100-PCIE-12GB\n",
      "   >>> [GLOBAL] FP32 | Epoch 1: Acc=0.7153 | Throughput=1176.4 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu2_batch128_fp32.pth ...\n",
      "   >>> [GLOBAL] FP32 | Epoch 2: Acc=0.7528 | Throughput=1169.9 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu2_batch128_fp32.pth ...\n",
      "   >>> [GLOBAL] FP32 | Epoch 3: Acc=0.7620 | Throughput=1177.4 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu2_batch128_fp32.pth ...\n",
      "   >>> [GLOBAL] FP32 | Epoch 4: Acc=0.8090 | Throughput=1191.8 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu2_batch128_fp32.pth ...\n",
      "   >>> [GLOBAL] FP32 | Epoch 5: Acc=0.8226 | Throughput=1220.0 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu2_batch128_fp32.pth ...\n",
      "   >>> [GLOBAL] FP32 | Epoch 6: Acc=0.8287 | Throughput=1264.7 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu2_batch128_fp32.pth ...\n",
      "   >>> [GLOBAL] FP32 | Epoch 7: Acc=0.8366 | Throughput=1268.0 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu2_batch128_fp32.pth ...\n",
      "   >>> [GLOBAL] FP32 | Epoch 8: Acc=0.8401 | Throughput=1254.9 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu2_batch128_fp32.pth ...\n",
      "   >>> [GLOBAL] FP32 | Epoch 9: Acc=0.8455 | Throughput=1190.7 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu2_batch128_fp32.pth ...\n",
      "   >>> [GLOBAL] FP32 | Epoch 10: Acc=0.8479 | Throughput=1187.2 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu2_batch128_fp32.pth ...\n",
      "\n",
      "--- Running: 2 GPU(s) | Batch 512 | DDP (Standard) | Port 42399 ---\n",
      "   [Rank 0] Ready | FP32 | Tesla P100-PCIE-12GB\n",
      "   >>> [GLOBAL] FP32 | Epoch 1: Acc=0.7703 | Throughput=1131.8 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu2_batch512_fp32.pth ...\n",
      "   >>> [GLOBAL] FP32 | Epoch 2: Acc=0.8311 | Throughput=1181.9 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu2_batch512_fp32.pth ...\n",
      "   >>> [GLOBAL] FP32 | Epoch 3: Acc=0.8511 | Throughput=1218.4 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu2_batch512_fp32.pth ...\n",
      "   >>> [GLOBAL] FP32 | Epoch 4: Acc=0.8600 | Throughput=1241.2 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu2_batch512_fp32.pth ...\n",
      "   >>> [GLOBAL] FP32 | Epoch 5: Acc=0.8682 | Throughput=1185.7 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu2_batch512_fp32.pth ...\n",
      "   >>> [GLOBAL] FP32 | Epoch 6: Acc=0.8769 | Throughput=1193.6 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu2_batch512_fp32.pth ...\n",
      "   >>> [GLOBAL] FP32 | Epoch 7: Acc=0.8820 | Throughput=1178.0 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu2_batch512_fp32.pth ...\n",
      "   >>> [GLOBAL] FP32 | Epoch 8: Acc=0.8861 | Throughput=1193.5 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu2_batch512_fp32.pth ...\n",
      "   >>> [GLOBAL] FP32 | Epoch 9: Acc=0.8905 | Throughput=1201.5 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu2_batch512_fp32.pth ...\n",
      "   >>> [GLOBAL] FP32 | Epoch 10: Acc=0.8961 | Throughput=1210.7 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu2_batch512_fp32.pth ...\n",
      "\n",
      "--- Running: 4 GPU(s) | Batch 64 | DDP (Standard) | Port 42335 ---\n",
      "   [Rank 0] Ready | FP32 | Tesla P100-PCIE-12GB\n",
      "   >>> [GLOBAL] FP32 | Epoch 1: Acc=0.8169 | Throughput=2270.7 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu4_batch64_fp32.pth ...\n",
      "   >>> [GLOBAL] FP32 | Epoch 2: Acc=0.8497 | Throughput=2244.8 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu4_batch64_fp32.pth ...\n",
      "   >>> [GLOBAL] FP32 | Epoch 3: Acc=0.8641 | Throughput=2253.6 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu4_batch64_fp32.pth ...\n",
      "   >>> [GLOBAL] FP32 | Epoch 4: Acc=0.8769 | Throughput=2267.7 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu4_batch64_fp32.pth ...\n",
      "   >>> [GLOBAL] FP32 | Epoch 5: Acc=0.8878 | Throughput=2279.3 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu4_batch64_fp32.pth ...\n",
      "   >>> [GLOBAL] FP32 | Epoch 6: Acc=0.8982 | Throughput=2303.0 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu4_batch64_fp32.pth ...\n",
      "   >>> [GLOBAL] FP32 | Epoch 7: Acc=0.9052 | Throughput=2406.0 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu4_batch64_fp32.pth ...\n",
      "   >>> [GLOBAL] FP32 | Epoch 8: Acc=0.9092 | Throughput=2406.1 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu4_batch64_fp32.pth ...\n",
      "   >>> [GLOBAL] FP32 | Epoch 9: Acc=0.9144 | Throughput=2404.7 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu4_batch64_fp32.pth ...\n",
      "   >>> [GLOBAL] FP32 | Epoch 10: Acc=0.9183 | Throughput=2403.8 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu4_batch64_fp32.pth ...\n",
      "\n",
      "--- Running: 4 GPU(s) | Batch 128 | DDP (Standard) | Port 41851 ---\n",
      "   [Rank 0] Ready | FP32 | Tesla P100-PCIE-12GB\n",
      "   >>> [GLOBAL] FP32 | Epoch 1: Acc=0.8020 | Throughput=2413.6 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu4_batch128_fp32.pth ...\n",
      "   >>> [GLOBAL] FP32 | Epoch 2: Acc=0.8469 | Throughput=2352.5 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu4_batch128_fp32.pth ...\n",
      "   >>> [GLOBAL] FP32 | Epoch 3: Acc=0.8607 | Throughput=2366.1 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu4_batch128_fp32.pth ...\n",
      "   >>> [GLOBAL] FP32 | Epoch 4: Acc=0.8710 | Throughput=2348.3 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu4_batch128_fp32.pth ...\n",
      "   >>> [GLOBAL] FP32 | Epoch 5: Acc=0.8810 | Throughput=2368.1 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu4_batch128_fp32.pth ...\n",
      "   >>> [GLOBAL] FP32 | Epoch 6: Acc=0.8882 | Throughput=2333.1 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu4_batch128_fp32.pth ...\n",
      "   >>> [GLOBAL] FP32 | Epoch 7: Acc=0.8958 | Throughput=2346.7 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu4_batch128_fp32.pth ...\n",
      "   >>> [GLOBAL] FP32 | Epoch 8: Acc=0.8995 | Throughput=2377.6 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu4_batch128_fp32.pth ...\n",
      "   >>> [GLOBAL] FP32 | Epoch 9: Acc=0.9043 | Throughput=2390.3 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu4_batch128_fp32.pth ...\n",
      "   >>> [GLOBAL] FP32 | Epoch 10: Acc=0.9078 | Throughput=2399.5 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu4_batch128_fp32.pth ...\n",
      "\n",
      "--- Running: 4 GPU(s) | Batch 512 | DDP (Standard) | Port 52629 ---\n",
      "   [Rank 0] Ready | FP32 | Tesla P100-PCIE-12GB\n",
      "   >>> [GLOBAL] FP32 | Epoch 1: Acc=0.7551 | Throughput=2401.2 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu4_batch512_fp32.pth ...\n",
      "   >>> [GLOBAL] FP32 | Epoch 2: Acc=0.8153 | Throughput=2506.1 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu4_batch512_fp32.pth ...\n",
      "   >>> [GLOBAL] FP32 | Epoch 3: Acc=0.8361 | Throughput=2352.0 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu4_batch512_fp32.pth ...\n",
      "   >>> [GLOBAL] FP32 | Epoch 4: Acc=0.8447 | Throughput=2375.2 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu4_batch512_fp32.pth ...\n",
      "   >>> [GLOBAL] FP32 | Epoch 5: Acc=0.8520 | Throughput=2383.5 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu4_batch512_fp32.pth ...\n",
      "   >>> [GLOBAL] FP32 | Epoch 6: Acc=0.8596 | Throughput=2361.4 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu4_batch512_fp32.pth ...\n",
      "   >>> [GLOBAL] FP32 | Epoch 7: Acc=0.8643 | Throughput=2388.9 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu4_batch512_fp32.pth ...\n",
      "   >>> [GLOBAL] FP32 | Epoch 8: Acc=0.8703 | Throughput=2364.8 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu4_batch512_fp32.pth ...\n",
      "   >>> [GLOBAL] FP32 | Epoch 9: Acc=0.8753 | Throughput=2375.6 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu4_batch512_fp32.pth ...\n",
      "   >>> [GLOBAL] FP32 | Epoch 10: Acc=0.8810 | Throughput=2372.7 img/s\n",
      "   ... Saving checkpoint to ckpt_gpu4_batch512_fp32.pth ...\n",
      "\n",
      "Saving Master CSV...\n",
      "Saved to 'benchmark_master_results_for_ddp.csv'\n",
      "  precision  gpu_count  ...              gpu_name  each_gpu_memory\n",
      "0      FP32          1  ...  Tesla P100-PCIE-12GB          12194MB\n",
      "1      FP32          1  ...  Tesla P100-PCIE-12GB          12194MB\n",
      "2      FP32          1  ...  Tesla P100-PCIE-12GB          12194MB\n",
      "3      FP32          1  ...  Tesla P100-PCIE-12GB          12194MB\n",
      "4      FP32          1  ...  Tesla P100-PCIE-12GB          12194MB\n",
      "5      FP32          1  ...  Tesla P100-PCIE-12GB          12194MB\n",
      "6      FP32          1  ...  Tesla P100-PCIE-12GB          12194MB\n",
      "7      FP32          1  ...  Tesla P100-PCIE-12GB          12194MB\n",
      "8      FP32          1  ...  Tesla P100-PCIE-12GB          12194MB\n",
      "9      FP32          2  ...  Tesla P100-PCIE-12GB          12194MB\n",
      "\n",
      "[10 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "!python ddp_benchmark.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (hpc)",
   "language": "python",
   "name": "hpc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
