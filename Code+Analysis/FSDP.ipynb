{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5a5f2d5-0223-4ec5-9773-0b8b3ea181da",
   "metadata": {},
   "source": [
    "# FullySharedDataParallel (FSDP) Model Training\n",
    "\n",
    "FSDP focus on **How we split/shard model parameters and optimizer state across GPUs**\n",
    "\n",
    "### What this FSDP script does?\n",
    "High-level design:\n",
    "- Uses `mp.spawn` to lunch one process per GPU. <br>\n",
    "- Each process:\n",
    "    * Initilizes distributed with `init_process_group`\n",
    "    * Wraps the model with FSDP\n",
    "    * Uses a **DistributedSampler** for train/val sets. <br> <br>\n",
    "- Runs for:\n",
    "    * `gpu_count` in `[1, 2, 4]`\n",
    "    * `batch_size` in `[64, 128, 512]`\n",
    "    * `mode` in `[\"fsdp\", \"fsdp_amp\"]` (plain FSDP vs FSDP+AMP)\n",
    "    * `num_epochs = 10` <br><br>\n",
    "- Per epoch, on each config, it:\n",
    "    * Measures **epoch wall-clock time.**\n",
    "    * Tracks **train loss, train accuracy, val loss, val accuracy.**\n",
    "    * Measures peak **GPU Memory** on each rank with:\n",
    "        * `torch.cuda.reset_peak_memory_stats(device)` at start of epoch\n",
    "        * `torch.cuda.max_memory_allocated(device)` at end. <br><br>\n",
    "- Aggregates metrics across all ranks (GPUs) using `all_reduce`:\n",
    "    * Sums loss and counts to get **global** accuracy and loss.\n",
    "    * Uses `all reduce(..., op=MAX)` to get **global peak memory in bytes.** <br><br>\n",
    "- Rank 0:\n",
    "    * Appends per-epoch metrics to `shared_metrics` list (multiprocessing Manager).\n",
    "    * Prints nice logs: global acc, loss, epoch time, peak memory. <br><br>\n",
    "- After all runs finish:\n",
    "    * Convers `shared_metrics` to a DataFrame.\n",
    "    * Save to `fsdp_benckmark_metrics.csv` and prints a summary DataFrame at the end.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0adf104b-dff7-49a9-8d60-18371b2e8a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting fsdp_benchmark_with_metrics_report.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile fsdp_benchmark_with_metrics_report.py\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.multiprocessing as mp\n",
    "import torch.distributed as dist\n",
    "\n",
    "from torch.nn import Module\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.distributed.fsdp import FullyShardedDataParallel as FSDP, ShardingStrategy\n",
    "\n",
    "# AMP (new API if available, else fallback)\n",
    "try:\n",
    "    from torch.amp import autocast, GradScaler\n",
    "    USE_NEW_AMP = True\n",
    "except ImportError:\n",
    "    from torch.cuda.amp import autocast, GradScaler\n",
    "    USE_NEW_AMP = False\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1. DISTRIBUTED SETUP / CLEANUP\n",
    "# ============================================================\n",
    "\n",
    "def setup_dist(rank, world_size):\n",
    "    \"\"\"\n",
    "    Initialize the distributed process group for FSDP.\n",
    "    \"\"\"\n",
    "    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "    os.environ[\"MASTER_PORT\"] = \"12355\"\n",
    "    dist.init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n",
    "    torch.cuda.set_device(rank)\n",
    "\n",
    "\n",
    "def cleanup_dist():\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. DATASET & MODEL DEFINITION\n",
    "# ============================================================\n",
    "\n",
    "class OralCancerDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset:\n",
    "    - dataframe has columns: 'id', 'label'\n",
    "    - path_map maps 'id' -> full image path\n",
    "    \"\"\"\n",
    "    def __init__(self, dataframe, path_map, transform=None):\n",
    "        self.data = dataframe.reset_index(drop=True)\n",
    "        self.path_map = path_map\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.data.iloc[idx][\"id\"]\n",
    "        label = int(self.data.iloc[idx][\"label\"])\n",
    "\n",
    "        img_path = self.path_map.get(img_id)\n",
    "        if img_path is None:\n",
    "            image = Image.new(\"RGB\", (96, 96))\n",
    "        else:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "class SimpleCNN(Module):\n",
    "    \"\"\"\n",
    "    Simple CNN:\n",
    "    - 3 conv blocks (Conv -> BN -> ReLU -> MaxPool)\n",
    "    - Flatten -> FC(256) -> Dropout -> FC(1)\n",
    "    - Paired with BCEWithLogitsLoss for binary classification (0/1).\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(128 * 12 * 12, 256)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = x.view(-1, 128 * 12 * 12)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. FSDP TRAINING FUNCTION (PER PROCESS / PER GPU)\n",
    "# ============================================================\n",
    "\n",
    "def fsdp_train_process(\n",
    "    rank, world_size, df, path_map, batch_size, num_epochs, mode, csv_path\n",
    "):\n",
    "    \"\"\"\n",
    "    rank      : process rank (0 .. world_size-1)\n",
    "    world_size: number of GPUs/processes\n",
    "    df        : full dataframe with columns ['id', 'label']\n",
    "    path_map  : dict mapping id -> image path\n",
    "    batch_size: batch size per GPU\n",
    "    num_epochs: number of training epochs\n",
    "    mode      : 'fsdp' or 'fsdp_amp'\n",
    "    csv_path  : path to the CSV file to append metrics\n",
    "    \"\"\"\n",
    "    setup_dist(rank, world_size)\n",
    "    device = torch.device(f\"cuda:{rank}\")\n",
    "\n",
    "    # ---------- Data transforms ----------\n",
    "    IMG_SIZE = 96\n",
    "    train_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5] * 3, [0.5] * 3),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    val_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5] * 3, [0.5] * 3),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # ---------- Train/Val split ----------\n",
    "    train_df, val_df = train_test_split(\n",
    "        df,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=df[\"label\"],\n",
    "    )\n",
    "\n",
    "    train_dataset = OralCancerDataset(train_df, path_map, transform=train_transforms)\n",
    "    val_dataset = OralCancerDataset(val_df, path_map, transform=val_transforms)\n",
    "\n",
    "    train_sampler = DistributedSampler(\n",
    "        train_dataset, num_replicas=world_size, rank=rank, shuffle=True\n",
    "    )\n",
    "    val_sampler = DistributedSampler(\n",
    "        val_dataset, num_replicas=world_size, rank=rank, shuffle=False\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,  # sampler does the shuffling\n",
    "        sampler=train_sampler,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        sampler=val_sampler,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    # ---------- Model, FSDP wrapper, optimizer, loss ----------\n",
    "    torch.cuda.set_device(device)\n",
    "    model = SimpleCNN().to(device)\n",
    "\n",
    "    # Choose sharding strategy explicitly to avoid warnings\n",
    "    if world_size == 1:\n",
    "        sharding_strategy = ShardingStrategy.NO_SHARD\n",
    "    else:\n",
    "        sharding_strategy = ShardingStrategy.FULL_SHARD\n",
    "\n",
    "    model = FSDP(model, device_id=device, sharding_strategy=sharding_strategy)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    scaler = GradScaler() if mode == \"fsdp_amp\" else None\n",
    "\n",
    "    # ---------- Checkpoint setup ----------\n",
    "    ckpt_dir = \"checkpoints\"\n",
    "    os.makedirs(ckpt_dir, exist_ok=True)\n",
    "    ckpt_name = f\"fsdp_{mode}_g{world_size}_b{batch_size}.pt\"\n",
    "    ckpt_path = os.path.join(ckpt_dir, ckpt_name)\n",
    "\n",
    "    start_epoch = 0\n",
    "    if os.path.exists(ckpt_path):\n",
    "        # Load checkpoint on every rank (simpler for FSDP)\n",
    "        checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint[\"model_state\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
    "        if scaler is not None and checkpoint.get(\"scaler_state\", None) is not None:\n",
    "            scaler.load_state_dict(checkpoint[\"scaler_state\"])\n",
    "        start_epoch = checkpoint.get(\"epoch\", 0)\n",
    "        if rank == 0:\n",
    "            print(\n",
    "                f\"[RESUME] Mode={mode}, GPUs={world_size}, Batch={batch_size} \"\n",
    "                f\"from epoch {start_epoch}\"\n",
    "            )\n",
    "    else:\n",
    "        if rank == 0:\n",
    "            print(\n",
    "                f\"[START] Mode={mode}, GPUs={world_size}, Batch={batch_size}, \"\n",
    "                f\"{num_epochs} epochs\"\n",
    "            )\n",
    "\n",
    "    run_start_time = time.time()\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        train_sampler.set_epoch(epoch)\n",
    "\n",
    "        # Reset peak memory stats at the start of each epoch\n",
    "        torch.cuda.reset_peak_memory_stats(device)\n",
    "\n",
    "        # --------------- TRAINING PHASE ---------------\n",
    "        model.train()\n",
    "        epoch_start = time.time()\n",
    "\n",
    "        train_loss_sum = 0.0\n",
    "        train_correct_sum = 0\n",
    "        train_total = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            labels = labels.float().unsqueeze(1).to(device, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if mode == \"fsdp_amp\" and scaler is not None:\n",
    "                # AMP-enabled forward + backward\n",
    "                if USE_NEW_AMP:\n",
    "                    amp_ctx = autocast(device_type=\"cuda\")\n",
    "                else:\n",
    "                    amp_ctx = autocast()\n",
    "\n",
    "                with amp_ctx:\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            train_loss_sum += loss.item() * images.size(0)\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            preds = (probs >= 0.5).float()\n",
    "            train_correct_sum += (preds == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "\n",
    "        epoch_time = time.time() - epoch_start\n",
    "\n",
    "        # --------------- VALIDATION PHASE ---------------\n",
    "        model.eval()\n",
    "        val_loss_sum = 0.0\n",
    "        val_correct_sum = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images = images.to(device, non_blocking=True)\n",
    "                labels = labels.float().unsqueeze(1).to(device, non_blocking=True)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss_sum += loss.item() * images.size(0)\n",
    "                probs = torch.sigmoid(outputs)\n",
    "                preds = (probs >= 0.5).float()\n",
    "                val_correct_sum += (preds == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "        # --------------- METRIC AGGREGATION ---------------\n",
    "        metrics_tensor = torch.tensor(\n",
    "            [\n",
    "                train_loss_sum,\n",
    "                train_correct_sum,\n",
    "                train_total,\n",
    "                val_loss_sum,\n",
    "                val_correct_sum,\n",
    "                val_total,\n",
    "            ],\n",
    "            device=device,\n",
    "        )\n",
    "        dist.all_reduce(metrics_tensor, op=dist.ReduceOp.SUM)\n",
    "\n",
    "        # Peak memory (MAX across ranks)\n",
    "        peak_mem_bytes_local = torch.cuda.max_memory_allocated(device)\n",
    "        mem_tensor = torch.tensor(\n",
    "            peak_mem_bytes_local, device=device, dtype=torch.float64\n",
    "        )\n",
    "        dist.all_reduce(mem_tensor, op=dist.ReduceOp.MAX)\n",
    "        peak_mem_bytes_global = mem_tensor.item()\n",
    "        peak_mem_gb_global = peak_mem_bytes_global / (1024 ** 3)\n",
    "\n",
    "        # Unpack global metrics safely\n",
    "        global_train_loss = metrics_tensor[0].item() / max(\n",
    "            metrics_tensor[2].item(), 1\n",
    "        )\n",
    "        global_train_acc = metrics_tensor[1].item() / max(\n",
    "            metrics_tensor[2].item(), 1\n",
    "        )\n",
    "        global_val_loss = metrics_tensor[3].item() / max(\n",
    "            metrics_tensor[5].item(), 1\n",
    "        )\n",
    "        global_val_acc = metrics_tensor[4].item() / max(\n",
    "            metrics_tensor[5].item(), 1\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"[Rank {rank}] Epoch {epoch+1}/{num_epochs} \"\n",
    "            f\"LocalTrainAcc={(train_correct_sum / max(train_total,1)):.4f}\"\n",
    "        )\n",
    "\n",
    "        # --------------- SAVE CHECKPOINT & LOG (RANK 0) ---------------\n",
    "        if rank == 0:\n",
    "            # Save checkpoint so we can resume this config later\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"epoch\": epoch + 1,\n",
    "                    \"model_state\": model.state_dict(),\n",
    "                    \"optimizer_state\": optimizer.state_dict(),\n",
    "                    \"scaler_state\": scaler.state_dict() if scaler is not None else None,\n",
    "                },\n",
    "                ckpt_path,\n",
    "            )\n",
    "\n",
    "            print(\n",
    "                f\"   >>> [Mode={mode.upper()}] Epoch {epoch+1}/{num_epochs} \"\n",
    "                f\"| TrainAcc={global_train_acc:.4f} ValAcc={global_val_acc:.4f} \"\n",
    "                f\"| EpochTime={epoch_time:.2f}s | PeakMem={peak_mem_gb_global:.2f} GB\"\n",
    "            )\n",
    "\n",
    "            # Append metrics row to CSV (survives crashes)\n",
    "            row = {\n",
    "                \"mode\": mode,\n",
    "                \"gpu_count\": world_size,\n",
    "                \"batch_size\": batch_size,\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"train_loss\": global_train_loss,\n",
    "                \"train_acc\": global_train_acc,\n",
    "                \"val_loss\": global_val_loss,\n",
    "                \"val_acc\": global_val_acc,\n",
    "                \"epoch_time\": epoch_time,\n",
    "                \"peak_mem_bytes\": peak_mem_bytes_global,\n",
    "                \"peak_mem_gb\": peak_mem_gb_global,\n",
    "            }\n",
    "            file_exists = os.path.exists(csv_path)\n",
    "            pd.DataFrame([row]).to_csv(\n",
    "                csv_path, mode=\"a\", header=not file_exists, index=False\n",
    "            )\n",
    "\n",
    "    total_run_time = time.time() - run_start_time\n",
    "    if rank == 0:\n",
    "        print(\n",
    "            f\"\\n[SUMMARY][Mode={mode.upper()}] [GPUs={world_size}] [Batch={batch_size}] \"\n",
    "            f\"Total time for {num_epochs} epochs (including resume): {total_run_time:.2f}s\\n\"\n",
    "        )\n",
    "\n",
    "    cleanup_dist()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4. MAIN EXECUTION LOOP\n",
    "# ============================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mp.set_start_method(\"spawn\", force=True)\n",
    "\n",
    "    print(\"Loading dataset for FSDP benchmark...\")\n",
    "\n",
    "    # ------------- Load CSV (must have 'id', 'label' columns) -------------\n",
    "    df = pd.read_csv(\"oral_cancer_balanced.csv\")\n",
    "\n",
    "    # ------------- Build id -> path map (similar to your DDP script) -------------\n",
    "    path_map = {}\n",
    "    for root, dirs, files in os.walk(\"Data\"):\n",
    "        if \"val\" in dirs:\n",
    "            dirs.remove(\"val\")\n",
    "        for file in files:\n",
    "            if file.lower().endswith((\".jpg\", \".jpeg\", \".png\")) and \"val\" not in root:\n",
    "                path_map[file] = os.path.join(root, file)\n",
    "\n",
    "    max_gpus_available = torch.cuda.device_count()\n",
    "    print(f\"Available GPUs: {max_gpus_available}\")\n",
    "\n",
    "    # You can tweak which GPU counts & batch sizes to test\n",
    "    target_gpu_counts = [1, 2, 4]\n",
    "    gpu_counts = [g for g in target_gpu_counts if g <= max_gpus_available]\n",
    "\n",
    "    batch_sizes = [64, 128, 512]\n",
    "    num_epochs = 10\n",
    "\n",
    "    modes = [\"fsdp\", \"fsdp_amp\"]  # plain FSDP vs FSDP + AMP\n",
    "    csv_path = \"fsdp_benchmark_metrics.csv\"\n",
    "\n",
    "    print(\"Starting FSDP benchmark (with checkpointing + AMP comparison)...\")\n",
    "\n",
    "    for mode in modes:\n",
    "        for n_gpus in gpu_counts:\n",
    "            for b_size in batch_sizes:\n",
    "                print(\n",
    "                    f\"\\n=== Running Mode={mode.upper()}, GPUs={n_gpus}, BatchSize={b_size} ===\"\n",
    "                )\n",
    "                run_start = time.time()\n",
    "                try:\n",
    "                    mp.spawn(\n",
    "                        fsdp_train_process,\n",
    "                        args=(n_gpus, df, path_map, b_size, num_epochs, mode, csv_path),\n",
    "                        nprocs=n_gpus,\n",
    "                        join=True,\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"[ERROR] Mode={mode}, GPUs={n_gpus}, Batch={b_size} :: {e}\")\n",
    "                run_total = time.time() - run_start\n",
    "                print(\n",
    "                    f\"*** Completed Mode={mode.upper()}, GPUs={n_gpus}, Batch={b_size} \"\n",
    "                    f\"in {run_total:.2f}s ***\"\n",
    "                )\n",
    "\n",
    "    print(\"\\nFSDP benchmark completed. Metrics saved in 'fsdp_benchmark_metrics.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b256e82-1c31-4aca-9820-f618e31eea13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset for FSDP benchmark...\n",
      "Available GPUs: 4\n",
      "Starting FSDP benchmark (with checkpointing + AMP comparison)...\n",
      "\n",
      "=== Running Mode=FSDP, GPUs=1, BatchSize=64 ===\n",
      "/home/padala.r/MyProject/fsdp_benchmark_with_metrics_report.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path, map_location=device)\n",
      "/opt/miniconda/envs/eai/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:822: UserWarning: When using ``NO_SHARD`` for ``ShardingStrategy``, full_state_dict willbe returned.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/eai/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:859: UserWarning: When using ``NO_SHARD`` for ``ShardingStrategy``, full_state_dict willbe returned.\n",
      "  warnings.warn(\n",
      "[RESUME] Mode=fsdp, GPUs=1, Batch=64 from epoch 10\n",
      "\n",
      "[SUMMARY][Mode=FSDP] [GPUs=1] [Batch=64] Total time for 10 epochs (including resume): 0.00s\n",
      "\n",
      "*** Completed Mode=FSDP, GPUs=1, Batch=64 in 5.86s ***\n",
      "\n",
      "=== Running Mode=FSDP, GPUs=1, BatchSize=128 ===\n",
      "/home/padala.r/MyProject/fsdp_benchmark_with_metrics_report.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path, map_location=device)\n",
      "/opt/miniconda/envs/eai/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:822: UserWarning: When using ``NO_SHARD`` for ``ShardingStrategy``, full_state_dict willbe returned.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/eai/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:859: UserWarning: When using ``NO_SHARD`` for ``ShardingStrategy``, full_state_dict willbe returned.\n",
      "  warnings.warn(\n",
      "[RESUME] Mode=fsdp, GPUs=1, Batch=128 from epoch 10\n",
      "\n",
      "[SUMMARY][Mode=FSDP] [GPUs=1] [Batch=128] Total time for 10 epochs (including resume): 0.00s\n",
      "\n",
      "*** Completed Mode=FSDP, GPUs=1, Batch=128 in 5.28s ***\n",
      "\n",
      "=== Running Mode=FSDP, GPUs=1, BatchSize=512 ===\n",
      "/home/padala.r/MyProject/fsdp_benchmark_with_metrics_report.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path, map_location=device)\n",
      "/opt/miniconda/envs/eai/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:822: UserWarning: When using ``NO_SHARD`` for ``ShardingStrategy``, full_state_dict willbe returned.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/eai/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:859: UserWarning: When using ``NO_SHARD`` for ``ShardingStrategy``, full_state_dict willbe returned.\n",
      "  warnings.warn(\n",
      "[RESUME] Mode=fsdp, GPUs=1, Batch=512 from epoch 10\n",
      "\n",
      "[SUMMARY][Mode=FSDP] [GPUs=1] [Batch=512] Total time for 10 epochs (including resume): 0.00s\n",
      "\n",
      "*** Completed Mode=FSDP, GPUs=1, Batch=512 in 5.24s ***\n",
      "\n",
      "=== Running Mode=FSDP, GPUs=2, BatchSize=64 ===\n",
      "/home/padala.r/MyProject/fsdp_benchmark_with_metrics_report.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path, map_location=device)\n",
      "/home/padala.r/MyProject/fsdp_benchmark_with_metrics_report.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path, map_location=device)\n",
      "[RESUME] Mode=fsdp, GPUs=2, Batch=64 from epoch 10\n",
      "\n",
      "[SUMMARY][Mode=FSDP] [GPUs=2] [Batch=64] Total time for 10 epochs (including resume): 0.00s\n",
      "\n",
      "*** Completed Mode=FSDP, GPUs=2, Batch=64 in 10.47s ***\n",
      "\n",
      "=== Running Mode=FSDP, GPUs=2, BatchSize=128 ===\n",
      "/home/padala.r/MyProject/fsdp_benchmark_with_metrics_report.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path, map_location=device)\n",
      "/home/padala.r/MyProject/fsdp_benchmark_with_metrics_report.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path, map_location=device)\n",
      "[RESUME] Mode=fsdp, GPUs=2, Batch=128 from epoch 10\n",
      "\n",
      "[SUMMARY][Mode=FSDP] [GPUs=2] [Batch=128] Total time for 10 epochs (including resume): 0.00s\n",
      "\n",
      "[E1201 01:05:34.230244302 ProcessGroupNCCL.cpp:542] [Rank 1] Collective WorkNCCL(SeqNum=1, OpType=_ALLGATHER_BASE, NumelIn=2406401, NumelOut=4812802, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5\n",
      "ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. \n",
      "Last error:\n",
      "\n",
      "Exception raised from checkForNCCLErrorsInternal at /opt/conda/conda-bld/pytorch_1729647352509/work/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x151b58f6b446 in /opt/miniconda/envs/eai/lib/python3.10/site-packages/torch/lib/libc10.so)\n",
      "frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x151afb226000 in /opt/miniconda/envs/eai/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x151afb22624c in /opt/miniconda/envs/eai/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x151afb226460 in /opt/miniconda/envs/eai/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x151afb22dbda in /opt/miniconda/envs/eai/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x151afb22f69d in /opt/miniconda/envs/eai/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #6: <unknown function> + 0x145c0 (0x151b5937f5c0 in /opt/miniconda/envs/eai/lib/python3.10/site-packages/torch/lib/libtorch.so)\n",
      "frame #7: <unknown function> + 0x94ac3 (0x151b6bc94ac3 in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "frame #8: <unknown function> + 0x126850 (0x151b6bd26850 in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n",
      "*** Completed Mode=FSDP, GPUs=2, Batch=128 in 10.32s ***\n",
      "\n",
      "=== Running Mode=FSDP, GPUs=2, BatchSize=512 ===\n",
      "/home/padala.r/MyProject/fsdp_benchmark_with_metrics_report.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path, map_location=device)\n",
      "/home/padala.r/MyProject/fsdp_benchmark_with_metrics_report.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path, map_location=device)\n",
      "[RESUME] Mode=fsdp, GPUs=2, Batch=512 from epoch 10\n",
      "\n",
      "[SUMMARY][Mode=FSDP] [GPUs=2] [Batch=512] Total time for 10 epochs (including resume): 0.00s\n",
      "\n",
      "[E1201 01:05:45.608439502 ProcessGroupNCCL.cpp:542] [Rank 0] Collective WorkNCCL(SeqNum=1, OpType=_ALLGATHER_BASE, NumelIn=2406401, NumelOut=4812802, Timeout(ms)=600000) raised the following async exception: NCCL error: unhandled system error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5\n",
      "ncclSystemError: System call (e.g. socket, malloc) or external library call failed or device error. \n",
      "Last error:\n",
      "\n",
      "Exception raised from checkForNCCLErrorsInternal at /opt/conda/conda-bld/pytorch_1729647352509/work/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2027 (most recent call first):\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x1538db56b446 in /opt/miniconda/envs/eai/lib/python3.10/site-packages/torch/lib/libc10.so)\n",
      "frame #1: c10d::ProcessGroupNCCL::checkForNCCLErrorsInternal(std::shared_ptr<c10d::NCCLComm>&) + 0x220 (0x153885626000 in /opt/miniconda/envs/eai/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #2: c10d::ProcessGroupNCCL::WorkNCCL::checkAndSetException() + 0x7c (0x15388562624c in /opt/miniconda/envs/eai/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #3: c10d::ProcessGroupNCCL::WorkNCCL::isCompleted() + 0x90 (0x153885626460 in /opt/miniconda/envs/eai/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #4: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1da (0x15388562dbda in /opt/miniconda/envs/eai/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #5: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x15388562f69d in /opt/miniconda/envs/eai/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #6: <unknown function> + 0x145c0 (0x1538de7495c0 in /opt/miniconda/envs/eai/lib/python3.10/site-packages/torch/lib/libtorch.so)\n",
      "frame #7: <unknown function> + 0x94ac3 (0x1538f5094ac3 in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "frame #8: <unknown function> + 0x126850 (0x1538f5126850 in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n",
      "*** Completed Mode=FSDP, GPUs=2, Batch=512 in 10.38s ***\n",
      "\n",
      "=== Running Mode=FSDP, GPUs=4, BatchSize=64 ===\n",
      "/home/padala.r/MyProject/fsdp_benchmark_with_metrics_report.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path, map_location=device)\n",
      "/home/padala.r/MyProject/fsdp_benchmark_with_metrics_report.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path, map_location=device)\n",
      "/home/padala.r/MyProject/fsdp_benchmark_with_metrics_report.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path, map_location=device)\n",
      "/home/padala.r/MyProject/fsdp_benchmark_with_metrics_report.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path, map_location=device)\n",
      "[RESUME] Mode=fsdp, GPUs=4, Batch=64 from epoch 10\n",
      "\n",
      "[SUMMARY][Mode=FSDP] [GPUs=4] [Batch=64] Total time for 10 epochs (including resume): 0.00s\n",
      "\n",
      "*** Completed Mode=FSDP, GPUs=4, Batch=64 in 18.80s ***\n",
      "\n",
      "=== Running Mode=FSDP, GPUs=4, BatchSize=128 ===\n",
      "/home/padala.r/MyProject/fsdp_benchmark_with_metrics_report.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path, map_location=device)\n",
      "/home/padala.r/MyProject/fsdp_benchmark_with_metrics_report.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path, map_location=device)\n",
      "/home/padala.r/MyProject/fsdp_benchmark_with_metrics_report.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path, map_location=device)\n",
      "/home/padala.r/MyProject/fsdp_benchmark_with_metrics_report.py:214: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path, map_location=device)\n",
      "[RESUME] Mode=fsdp, GPUs=4, Batch=128 from epoch 5\n",
      "[Rank 2] Epoch 6/10 LocalTrainAcc=0.8795\n",
      "[Rank 3] Epoch 6/10 LocalTrainAcc=0.8799\n",
      "[Rank 0] Epoch 6/10 LocalTrainAcc=0.8793\n",
      "[Rank 1] Epoch 6/10 LocalTrainAcc=0.8775\n",
      "   >>> [Mode=FSDP] Epoch 6/10 | TrainAcc=0.8791 ValAcc=0.8702 | EpochTime=117.77s | PeakMem=0.85 GB\n",
      "[Rank 1] Epoch 7/10 LocalTrainAcc=0.8883\n",
      "[Rank 3] Epoch 7/10 LocalTrainAcc=0.8893\n",
      "[Rank 2] Epoch 7/10 LocalTrainAcc=0.8907\n",
      "[Rank 0] Epoch 7/10 LocalTrainAcc=0.8900\n",
      "   >>> [Mode=FSDP] Epoch 7/10 | TrainAcc=0.8896 ValAcc=0.8830 | EpochTime=101.57s | PeakMem=0.85 GB\n",
      "[Rank 1] Epoch 8/10 LocalTrainAcc=0.8949\n",
      "[Rank 2] Epoch 8/10 LocalTrainAcc=0.8947\n",
      "[Rank 3] Epoch 8/10 LocalTrainAcc=0.8960\n",
      "[Rank 0] Epoch 8/10 LocalTrainAcc=0.8941\n",
      "   >>> [Mode=FSDP] Epoch 8/10 | TrainAcc=0.8949 ValAcc=0.8844 | EpochTime=101.41s | PeakMem=0.85 GB\n",
      "[Rank 3] Epoch 9/10 LocalTrainAcc=0.9008\n",
      "[Rank 2] Epoch 9/10 LocalTrainAcc=0.8980\n",
      "[Rank 1] Epoch 9/10 LocalTrainAcc=0.8990\n",
      "[Rank 0] Epoch 9/10 LocalTrainAcc=0.9004\n",
      "   >>> [Mode=FSDP] Epoch 9/10 | TrainAcc=0.8995 ValAcc=0.8919 | EpochTime=102.07s | PeakMem=0.85 GB\n",
      "[Rank 3] Epoch 10/10 LocalTrainAcc=0.9045\n",
      "[Rank 1] Epoch 10/10 LocalTrainAcc=0.9053\n",
      "[Rank 2] Epoch 10/10 LocalTrainAcc=0.9019\n",
      "[Rank 0] Epoch 10/10 LocalTrainAcc=0.9045\n",
      "   >>> [Mode=FSDP] Epoch 10/10 | TrainAcc=0.9040 ValAcc=0.9064 | EpochTime=102.23s | PeakMem=0.85 GB\n",
      "\n",
      "[SUMMARY][Mode=FSDP] [GPUs=4] [Batch=128] Total time for 10 epochs (including resume): 618.76s\n",
      "\n",
      "*** Completed Mode=FSDP, GPUs=4, Batch=128 in 637.61s ***\n",
      "\n",
      "=== Running Mode=FSDP, GPUs=4, BatchSize=512 ===\n",
      "[START] Mode=fsdp, GPUs=4, Batch=512, 10 epochs\n",
      "[Rank 3] Epoch 1/10 LocalTrainAcc=0.7368\n",
      "[Rank 1] Epoch 1/10 LocalTrainAcc=0.7388\n",
      "[Rank 2] Epoch 1/10 LocalTrainAcc=0.7373\n",
      "[Rank 0] Epoch 1/10 LocalTrainAcc=0.7356\n",
      "   >>> [Mode=FSDP] Epoch 1/10 | TrainAcc=0.7371 ValAcc=0.7860 | EpochTime=98.93s | PeakMem=3.13 GB\n",
      "[Rank 1] Epoch 2/10 LocalTrainAcc=0.7969\n",
      "[Rank 3] Epoch 2/10 LocalTrainAcc=0.7966\n",
      "[Rank 2] Epoch 2/10 LocalTrainAcc=0.7943\n",
      "[Rank 0] Epoch 2/10 LocalTrainAcc=0.7937\n",
      "   >>> [Mode=FSDP] Epoch 2/10 | TrainAcc=0.7954 ValAcc=0.8050 | EpochTime=96.74s | PeakMem=3.13 GB\n",
      "[Rank 3] Epoch 3/10 LocalTrainAcc=0.8253\n",
      "[Rank 1] Epoch 3/10 LocalTrainAcc=0.8240\n",
      "[Rank 2] Epoch 3/10 LocalTrainAcc=0.8251\n",
      "[Rank 0] Epoch 3/10 LocalTrainAcc=0.8242\n",
      "   >>> [Mode=FSDP] Epoch 3/10 | TrainAcc=0.8247 ValAcc=0.8205 | EpochTime=95.00s | PeakMem=3.13 GB\n",
      "[Rank 1] Epoch 4/10 LocalTrainAcc=0.8384\n",
      "[Rank 3] Epoch 4/10 LocalTrainAcc=0.8408\n",
      "[Rank 2] Epoch 4/10 LocalTrainAcc=0.8428\n",
      "[Rank 0] Epoch 4/10 LocalTrainAcc=0.8424\n",
      "   >>> [Mode=FSDP] Epoch 4/10 | TrainAcc=0.8411 ValAcc=0.8486 | EpochTime=95.45s | PeakMem=3.13 GB\n",
      "[Rank 3] Epoch 5/10 LocalTrainAcc=0.8497\n",
      "[Rank 1] Epoch 5/10 LocalTrainAcc=0.8482\n",
      "[Rank 0] Epoch 5/10 LocalTrainAcc=0.8513\n",
      "[Rank 2] Epoch 5/10 LocalTrainAcc=0.8508\n",
      "   >>> [Mode=FSDP] Epoch 5/10 | TrainAcc=0.8500 ValAcc=0.8493 | EpochTime=94.97s | PeakMem=3.13 GB\n",
      "[Rank 3] Epoch 6/10 LocalTrainAcc=0.8561\n",
      "[Rank 2] Epoch 6/10 LocalTrainAcc=0.8553\n",
      "[Rank 1] Epoch 6/10 LocalTrainAcc=0.8541\n",
      "[Rank 0] Epoch 6/10 LocalTrainAcc=0.8559\n",
      "   >>> [Mode=FSDP] Epoch 6/10 | TrainAcc=0.8553 ValAcc=0.8486 | EpochTime=94.83s | PeakMem=3.13 GB\n",
      "[Rank 1] Epoch 7/10 LocalTrainAcc=0.8615\n",
      "[Rank 3] Epoch 7/10 LocalTrainAcc=0.8628\n",
      "[Rank 2] Epoch 7/10 LocalTrainAcc=0.8641\n",
      "[Rank 0] Epoch 7/10 LocalTrainAcc=0.8662\n",
      "   >>> [Mode=FSDP] Epoch 7/10 | TrainAcc=0.8637 ValAcc=0.8489 | EpochTime=95.09s | PeakMem=3.13 GB\n",
      "[Rank 3] Epoch 8/10 LocalTrainAcc=0.8665\n",
      "[Rank 1] Epoch 8/10 LocalTrainAcc=0.8700\n",
      "[Rank 2] Epoch 8/10 LocalTrainAcc=0.8659\n",
      "[Rank 0] Epoch 8/10 LocalTrainAcc=0.8705\n",
      "   >>> [Mode=FSDP] Epoch 8/10 | TrainAcc=0.8682 ValAcc=0.8696 | EpochTime=95.76s | PeakMem=3.13 GB\n",
      "[Rank 0] Epoch 9/10 LocalTrainAcc=0.8733\n",
      "[Rank 3] Epoch 9/10 LocalTrainAcc=0.8722\n",
      "[Rank 1] Epoch 9/10 LocalTrainAcc=0.8759\n",
      "[Rank 2] Epoch 9/10 LocalTrainAcc=0.8729\n",
      "   >>> [Mode=FSDP] Epoch 9/10 | TrainAcc=0.8736 ValAcc=0.8850 | EpochTime=94.60s | PeakMem=3.13 GB\n",
      "[Rank 0] Epoch 10/10 LocalTrainAcc=0.8794\n",
      "[Rank 3] Epoch 10/10 LocalTrainAcc=0.8776\n",
      "[Rank 2] Epoch 10/10 LocalTrainAcc=0.8751\n",
      "[Rank 1] Epoch 10/10 LocalTrainAcc=0.8800\n",
      "   >>> [Mode=FSDP] Epoch 10/10 | TrainAcc=0.8780 ValAcc=0.8834 | EpochTime=94.88s | PeakMem=3.13 GB\n",
      "\n",
      "[SUMMARY][Mode=FSDP] [GPUs=4] [Batch=512] Total time for 10 epochs (including resume): 1133.37s\n",
      "\n",
      "*** Completed Mode=FSDP, GPUs=4, Batch=512 in 1151.74s ***\n",
      "\n",
      "=== Running Mode=FSDP_AMP, GPUs=1, BatchSize=64 ===\n",
      "[START] Mode=fsdp_amp, GPUs=1, Batch=64, 10 epochs\n",
      "[Rank 0] Epoch 2/10 LocalTrainAcc=0.8597\n",
      "   >>> [Mode=FSDP_AMP] Epoch 2/10 | TrainAcc=0.8597 ValAcc=0.8609 | EpochTime=379.25s | PeakMem=0.48 GB\n",
      "[Rank 0] Epoch 3/10 LocalTrainAcc=0.8818\n",
      "   >>> [Mode=FSDP_AMP] Epoch 3/10 | TrainAcc=0.8818 ValAcc=0.8878 | EpochTime=381.19s | PeakMem=0.48 GB\n",
      "[Rank 0] Epoch 4/10 LocalTrainAcc=0.8973\n",
      "   >>> [Mode=FSDP_AMP] Epoch 4/10 | TrainAcc=0.8973 ValAcc=0.8784 | EpochTime=379.25s | PeakMem=0.48 GB\n",
      "[Rank 0] Epoch 5/10 LocalTrainAcc=0.9097\n",
      "   >>> [Mode=FSDP_AMP] Epoch 5/10 | TrainAcc=0.9097 ValAcc=0.9179 | EpochTime=377.60s | PeakMem=0.48 GB\n",
      "[Rank 0] Epoch 6/10 LocalTrainAcc=0.9178\n",
      "   >>> [Mode=FSDP_AMP] Epoch 6/10 | TrainAcc=0.9178 ValAcc=0.9261 | EpochTime=376.94s | PeakMem=0.48 GB\n",
      "[Rank 0] Epoch 7/10 LocalTrainAcc=0.9239\n",
      "   >>> [Mode=FSDP_AMP] Epoch 7/10 | TrainAcc=0.9239 ValAcc=0.9066 | EpochTime=377.76s | PeakMem=0.48 GB\n",
      "[Rank 0] Epoch 8/10 LocalTrainAcc=0.9290\n",
      "   >>> [Mode=FSDP_AMP] Epoch 8/10 | TrainAcc=0.9290 ValAcc=0.9296 | EpochTime=376.90s | PeakMem=0.48 GB\n",
      "[Rank 0] Epoch 9/10 LocalTrainAcc=0.9332\n",
      "   >>> [Mode=FSDP_AMP] Epoch 9/10 | TrainAcc=0.9332 ValAcc=0.9311 | EpochTime=378.52s | PeakMem=0.48 GB\n",
      "[Rank 0] Epoch 10/10 LocalTrainAcc=0.9374\n",
      "   >>> [Mode=FSDP_AMP] Epoch 10/10 | TrainAcc=0.9374 ValAcc=0.9429 | EpochTime=378.18s | PeakMem=0.48 GB\n",
      "\n",
      "[SUMMARY][Mode=FSDP_AMP] [GPUs=1] [Batch=64] Total time for 10 epochs (including resume): 4458.93s\n",
      "\n",
      "*** Completed Mode=FSDP_AMP, GPUs=1, Batch=64 in 4464.71s ***\n",
      "\n",
      "=== Running Mode=FSDP_AMP, GPUs=1, BatchSize=128 ===\n",
      "[START] Mode=fsdp_amp, GPUs=1, Batch=128, 10 epochs\n",
      "[Rank 0] Epoch 2/10 LocalTrainAcc=0.8570\n",
      "   >>> [Mode=FSDP_AMP] Epoch 2/10 | TrainAcc=0.8570 ValAcc=0.8745 | EpochTime=372.35s | PeakMem=0.87 GB\n",
      "[Rank 0] Epoch 3/10 LocalTrainAcc=0.8734\n",
      "   >>> [Mode=FSDP_AMP] Epoch 3/10 | TrainAcc=0.8734 ValAcc=0.8809 | EpochTime=372.36s | PeakMem=0.87 GB\n",
      "[Rank 0] Epoch 4/10 LocalTrainAcc=0.8874\n",
      "   >>> [Mode=FSDP_AMP] Epoch 4/10 | TrainAcc=0.8874 ValAcc=0.8882 | EpochTime=369.37s | PeakMem=0.87 GB\n",
      "[Rank 0] Epoch 5/10 LocalTrainAcc=0.8992\n",
      "   >>> [Mode=FSDP_AMP] Epoch 5/10 | TrainAcc=0.8992 ValAcc=0.9088 | EpochTime=373.07s | PeakMem=0.87 GB\n",
      "[Rank 0] Epoch 7/10 LocalTrainAcc=0.9144\n",
      "   >>> [Mode=FSDP_AMP] Epoch 7/10 | TrainAcc=0.9144 ValAcc=0.9167 | EpochTime=369.67s | PeakMem=0.87 GB\n",
      "[Rank 0] Epoch 8/10 LocalTrainAcc=0.9201\n",
      "   >>> [Mode=FSDP_AMP] Epoch 8/10 | TrainAcc=0.9201 ValAcc=0.9179 | EpochTime=370.13s | PeakMem=0.87 GB\n",
      "[Rank 0] Epoch 9/10 LocalTrainAcc=0.9259\n",
      "   >>> [Mode=FSDP_AMP] Epoch 9/10 | TrainAcc=0.9259 ValAcc=0.9290 | EpochTime=369.83s | PeakMem=0.87 GB\n",
      "[Rank 0] Epoch 10/10 LocalTrainAcc=0.9290\n",
      "   >>> [Mode=FSDP_AMP] Epoch 10/10 | TrainAcc=0.9290 ValAcc=0.9217 | EpochTime=369.03s | PeakMem=0.87 GB\n",
      "\n",
      "[SUMMARY][Mode=FSDP_AMP] [GPUs=1] [Batch=128] Total time for 10 epochs (including resume): 4394.06s\n",
      "\n",
      "*** Completed Mode=FSDP_AMP, GPUs=1, Batch=128 in 4400.00s ***\n",
      "\n",
      "=== Running Mode=FSDP_AMP, GPUs=1, BatchSize=512 ===\n",
      "[START] Mode=fsdp_amp, GPUs=1, Batch=512, 10 epochs\n",
      "[Rank 0] Epoch 1/10 LocalTrainAcc=0.8039\n",
      "/opt/miniconda/envs/eai/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:768: UserWarning: When using ``NO_SHARD`` for ``ShardingStrategy``, full_state_dict willbe returned.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/eai/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:711: UserWarning: When using ``NO_SHARD`` for ``ShardingStrategy``, full_state_dict willbe returned.\n",
      "  warnings.warn(\n",
      "   >>> [Mode=FSDP_AMP] Epoch 1/10 | TrainAcc=0.8039 ValAcc=0.8343 | EpochTime=377.80s | PeakMem=3.23 GB\n",
      "[Rank 0] Epoch 2/10 LocalTrainAcc=0.8509\n",
      "   >>> [Mode=FSDP_AMP] Epoch 2/10 | TrainAcc=0.8509 ValAcc=0.8633 | EpochTime=379.59s | PeakMem=3.23 GB\n",
      "[Rank 0] Epoch 3/10 LocalTrainAcc=0.8659\n",
      "   >>> [Mode=FSDP_AMP] Epoch 3/10 | TrainAcc=0.8659 ValAcc=0.8760 | EpochTime=374.55s | PeakMem=3.23 GB\n",
      "[Rank 0] Epoch 4/10 LocalTrainAcc=0.8769\n",
      "   >>> [Mode=FSDP_AMP] Epoch 4/10 | TrainAcc=0.8769 ValAcc=0.8807 | EpochTime=377.01s | PeakMem=3.23 GB\n",
      "[Rank 0] Epoch 5/10 LocalTrainAcc=0.8833\n",
      "   >>> [Mode=FSDP_AMP] Epoch 5/10 | TrainAcc=0.8833 ValAcc=0.8641 | EpochTime=351.84s | PeakMem=3.23 GB\n",
      "[Rank 0] Epoch 6/10 LocalTrainAcc=0.8914\n",
      "   >>> [Mode=FSDP_AMP] Epoch 6/10 | TrainAcc=0.8914 ValAcc=0.8955 | EpochTime=327.77s | PeakMem=3.23 GB\n",
      "[Rank 0] Epoch 7/10 LocalTrainAcc=0.8987\n",
      "   >>> [Mode=FSDP_AMP] Epoch 7/10 | TrainAcc=0.8987 ValAcc=0.8794 | EpochTime=325.43s | PeakMem=3.23 GB\n",
      "[Rank 0] Epoch 8/10 LocalTrainAcc=0.9008\n",
      "   >>> [Mode=FSDP_AMP] Epoch 8/10 | TrainAcc=0.9008 ValAcc=0.8804 | EpochTime=320.49s | PeakMem=3.23 GB\n",
      "[Rank 0] Epoch 9/10 LocalTrainAcc=0.9066\n",
      "   >>> [Mode=FSDP_AMP] Epoch 9/10 | TrainAcc=0.9066 ValAcc=0.9135 | EpochTime=312.51s | PeakMem=3.23 GB\n",
      "[Rank 0] Epoch 10/10 LocalTrainAcc=0.9107\n",
      "   >>> [Mode=FSDP_AMP] Epoch 10/10 | TrainAcc=0.9107 ValAcc=0.8998 | EpochTime=313.66s | PeakMem=3.23 GB\n",
      "\n",
      "[SUMMARY][Mode=FSDP_AMP] [GPUs=1] [Batch=512] Total time for 10 epochs (including resume): 4117.94s\n",
      "\n",
      "*** Completed Mode=FSDP_AMP, GPUs=1, Batch=512 in 4123.79s ***\n",
      "\n",
      "=== Running Mode=FSDP_AMP, GPUs=2, BatchSize=64 ===\n",
      "[START] Mode=fsdp_amp, GPUs=2, Batch=64, 10 epochs\n",
      "[Rank 1] Epoch 1/10 LocalTrainAcc=0.5231\n",
      "[Rank 0] Epoch 1/10 LocalTrainAcc=0.5205\n",
      "   >>> [Mode=FSDP_AMP] Epoch 1/10 | TrainAcc=0.5218 ValAcc=0.5000 | EpochTime=174.07s | PeakMem=0.47 GB\n",
      "[Rank 1] Epoch 2/10 LocalTrainAcc=0.4993\n",
      "[Rank 0] Epoch 2/10 LocalTrainAcc=0.5007\n",
      "   >>> [Mode=FSDP_AMP] Epoch 2/10 | TrainAcc=0.5000 ValAcc=0.5000 | EpochTime=172.80s | PeakMem=0.47 GB\n",
      "[Rank 1] Epoch 3/10 LocalTrainAcc=0.4993\n",
      "[Rank 0] Epoch 3/10 LocalTrainAcc=0.5007\n",
      "   >>> [Mode=FSDP_AMP] Epoch 3/10 | TrainAcc=0.5000 ValAcc=0.5000 | EpochTime=174.22s | PeakMem=0.47 GB\n",
      "[Rank 1] Epoch 4/10 LocalTrainAcc=0.5010\n",
      "[Rank 0] Epoch 4/10 LocalTrainAcc=0.4990\n",
      "   >>> [Mode=FSDP_AMP] Epoch 4/10 | TrainAcc=0.5000 ValAcc=0.5000 | EpochTime=172.62s | PeakMem=0.47 GB\n",
      "[Rank 1] Epoch 5/10 LocalTrainAcc=0.4986\n",
      "[Rank 0] Epoch 5/10 LocalTrainAcc=0.5014\n",
      "   >>> [Mode=FSDP_AMP] Epoch 5/10 | TrainAcc=0.5000 ValAcc=0.5000 | EpochTime=172.95s | PeakMem=0.47 GB\n",
      "[Rank 1] Epoch 6/10 LocalTrainAcc=0.5006\n",
      "[Rank 0] Epoch 6/10 LocalTrainAcc=0.4994\n",
      "   >>> [Mode=FSDP_AMP] Epoch 6/10 | TrainAcc=0.5000 ValAcc=0.5000 | EpochTime=172.96s | PeakMem=0.47 GB\n",
      "[Rank 1] Epoch 7/10 LocalTrainAcc=0.4996\n",
      "[Rank 0] Epoch 7/10 LocalTrainAcc=0.5004\n",
      "   >>> [Mode=FSDP_AMP] Epoch 7/10 | TrainAcc=0.5000 ValAcc=0.5000 | EpochTime=172.24s | PeakMem=0.47 GB\n",
      "[Rank 1] Epoch 8/10 LocalTrainAcc=0.5005\n",
      "[Rank 0] Epoch 8/10 LocalTrainAcc=0.4995\n",
      "   >>> [Mode=FSDP_AMP] Epoch 8/10 | TrainAcc=0.5000 ValAcc=0.5000 | EpochTime=172.53s | PeakMem=0.47 GB\n",
      "[Rank 1] Epoch 9/10 LocalTrainAcc=0.4990\n",
      "[Rank 0] Epoch 9/10 LocalTrainAcc=0.5010\n",
      "   >>> [Mode=FSDP_AMP] Epoch 9/10 | TrainAcc=0.5000 ValAcc=0.5000 | EpochTime=172.66s | PeakMem=0.47 GB\n",
      "[Rank 1] Epoch 10/10 LocalTrainAcc=0.5017\n",
      "[Rank 0] Epoch 10/10 LocalTrainAcc=0.4983\n",
      "   >>> [Mode=FSDP_AMP] Epoch 10/10 | TrainAcc=0.5000 ValAcc=0.5000 | EpochTime=172.18s | PeakMem=0.47 GB\n",
      "\n",
      "[SUMMARY][Mode=FSDP_AMP] [GPUs=2] [Batch=64] Total time for 10 epochs (including resume): 2050.85s\n",
      "\n",
      "*** Completed Mode=FSDP_AMP, GPUs=2, Batch=64 in 2060.55s ***\n",
      "\n",
      "=== Running Mode=FSDP_AMP, GPUs=2, BatchSize=128 ===\n",
      "[START] Mode=fsdp_amp, GPUs=2, Batch=128, 10 epochs\n",
      "[Rank 0] Epoch 1/10 LocalTrainAcc=0.6941\n",
      "[Rank 1] Epoch 1/10 LocalTrainAcc=0.6961\n",
      "   >>> [Mode=FSDP_AMP] Epoch 1/10 | TrainAcc=0.6951 ValAcc=0.5000 | EpochTime=163.55s | PeakMem=0.86 GB\n",
      "[Rank 1] Epoch 2/10 LocalTrainAcc=0.4993\n",
      "[Rank 0] Epoch 2/10 LocalTrainAcc=0.5007\n",
      "   >>> [Mode=FSDP_AMP] Epoch 2/10 | TrainAcc=0.5000 ValAcc=0.5000 | EpochTime=161.70s | PeakMem=0.86 GB\n",
      "[Rank 1] Epoch 3/10 LocalTrainAcc=0.4993\n",
      "[Rank 0] Epoch 3/10 LocalTrainAcc=0.5007\n",
      "   >>> [Mode=FSDP_AMP] Epoch 3/10 | TrainAcc=0.5000 ValAcc=0.5000 | EpochTime=162.01s | PeakMem=0.86 GB\n",
      "[Rank 1] Epoch 4/10 LocalTrainAcc=0.5010\n",
      "[Rank 0] Epoch 4/10 LocalTrainAcc=0.4990\n",
      "   >>> [Mode=FSDP_AMP] Epoch 4/10 | TrainAcc=0.5000 ValAcc=0.5000 | EpochTime=161.80s | PeakMem=0.86 GB\n",
      "[Rank 0] Epoch 5/10 LocalTrainAcc=0.5014\n",
      "[Rank 1] Epoch 5/10 LocalTrainAcc=0.4986\n",
      "   >>> [Mode=FSDP_AMP] Epoch 5/10 | TrainAcc=0.5000 ValAcc=0.5000 | EpochTime=164.92s | PeakMem=0.86 GB\n",
      "[Rank 1] Epoch 6/10 LocalTrainAcc=0.5006\n",
      "[Rank 0] Epoch 6/10 LocalTrainAcc=0.4994\n",
      "   >>> [Mode=FSDP_AMP] Epoch 6/10 | TrainAcc=0.5000 ValAcc=0.5000 | EpochTime=165.33s | PeakMem=0.86 GB\n",
      "[Rank 1] Epoch 7/10 LocalTrainAcc=0.4996\n",
      "[Rank 0] Epoch 7/10 LocalTrainAcc=0.5004\n",
      "   >>> [Mode=FSDP_AMP] Epoch 7/10 | TrainAcc=0.5000 ValAcc=0.5000 | EpochTime=165.64s | PeakMem=0.86 GB\n",
      "[Rank 1] Epoch 8/10 LocalTrainAcc=0.5005\n",
      "[Rank 0] Epoch 8/10 LocalTrainAcc=0.4995\n",
      "   >>> [Mode=FSDP_AMP] Epoch 8/10 | TrainAcc=0.5000 ValAcc=0.5000 | EpochTime=164.88s | PeakMem=0.86 GB\n",
      "[Rank 1] Epoch 9/10 LocalTrainAcc=0.4990\n",
      "[Rank 0] Epoch 9/10 LocalTrainAcc=0.5010\n",
      "   >>> [Mode=FSDP_AMP] Epoch 9/10 | TrainAcc=0.5000 ValAcc=0.5000 | EpochTime=165.94s | PeakMem=0.86 GB\n",
      "[Rank 0] Epoch 10/10 LocalTrainAcc=0.4983\n",
      "[Rank 1] Epoch 10/10 LocalTrainAcc=0.5017\n",
      "   >>> [Mode=FSDP_AMP] Epoch 10/10 | TrainAcc=0.5000 ValAcc=0.5000 | EpochTime=167.85s | PeakMem=0.86 GB\n",
      "\n",
      "[SUMMARY][Mode=FSDP_AMP] [GPUs=2] [Batch=128] Total time for 10 epochs (including resume): 1961.40s\n",
      "\n",
      "*** Completed Mode=FSDP_AMP, GPUs=2, Batch=128 in 1970.83s ***\n",
      "\n",
      "=== Running Mode=FSDP_AMP, GPUs=2, BatchSize=512 ===\n",
      "[START] Mode=fsdp_amp, GPUs=2, Batch=512, 10 epochs\n",
      "[Rank 0] Epoch 1/10 LocalTrainAcc=0.7243\n",
      "[Rank 1] Epoch 1/10 LocalTrainAcc=0.7245\n",
      "   >>> [Mode=FSDP_AMP] Epoch 1/10 | TrainAcc=0.7244 ValAcc=0.8156 | EpochTime=166.64s | PeakMem=3.23 GB\n",
      "[Rank 0] Epoch 3/10 LocalTrainAcc=0.5007\n",
      "[Rank 1] Epoch 3/10 LocalTrainAcc=0.4993\n",
      "   >>> [Mode=FSDP_AMP] Epoch 3/10 | TrainAcc=0.5000 ValAcc=0.5000 | EpochTime=163.95s | PeakMem=3.23 GB\n",
      "[Rank 0] Epoch 5/10 LocalTrainAcc=0.5014\n",
      "[Rank 1] Epoch 5/10 LocalTrainAcc=0.4986\n",
      "   >>> [Mode=FSDP_AMP] Epoch 5/10 | TrainAcc=0.5000 ValAcc=0.5000 | EpochTime=161.90s | PeakMem=3.23 GB\n",
      "[Rank 0] Epoch 6/10 LocalTrainAcc=0.4994\n",
      "[Rank 1] Epoch 6/10 LocalTrainAcc=0.5006\n",
      "   >>> [Mode=FSDP_AMP] Epoch 6/10 | TrainAcc=0.5000 ValAcc=0.5000 | EpochTime=160.89s | PeakMem=3.23 GB\n",
      "[Rank 0] Epoch 7/10 LocalTrainAcc=0.5004\n",
      "[Rank 1] Epoch 7/10 LocalTrainAcc=0.4996\n",
      "   >>> [Mode=FSDP_AMP] Epoch 7/10 | TrainAcc=0.5000 ValAcc=0.5000 | EpochTime=163.07s | PeakMem=3.23 GB\n",
      "[Rank 0] Epoch 8/10 LocalTrainAcc=0.4995\n",
      "[Rank 1] Epoch 8/10 LocalTrainAcc=0.5005\n",
      "   >>> [Mode=FSDP_AMP] Epoch 8/10 | TrainAcc=0.5000 ValAcc=0.5000 | EpochTime=162.44s | PeakMem=3.23 GB\n",
      "[Rank 0] Epoch 9/10 LocalTrainAcc=0.5010\n",
      "[Rank 1] Epoch 9/10 LocalTrainAcc=0.4990\n",
      "   >>> [Mode=FSDP_AMP] Epoch 9/10 | TrainAcc=0.5000 ValAcc=0.5000 | EpochTime=161.22s | PeakMem=3.23 GB\n",
      "[Rank 0] Epoch 10/10 LocalTrainAcc=0.4983\n",
      "[Rank 1] Epoch 10/10 LocalTrainAcc=0.5017\n",
      "   >>> [Mode=FSDP_AMP] Epoch 10/10 | TrainAcc=0.5000 ValAcc=0.5000 | EpochTime=161.52s | PeakMem=3.23 GB\n",
      "\n",
      "[SUMMARY][Mode=FSDP_AMP] [GPUs=2] [Batch=512] Total time for 10 epochs (including resume): 1951.46s\n",
      "\n",
      "*** Completed Mode=FSDP_AMP, GPUs=2, Batch=512 in 1960.74s ***\n",
      "\n",
      "=== Running Mode=FSDP_AMP, GPUs=4, BatchSize=64 ===\n",
      "[Rank 0] Epoch 1/10 LocalTrainAcc=0.7816\n",
      "[Rank 1] Epoch 1/10 LocalTrainAcc=0.7802\n",
      "[Rank 3] Epoch 1/10 LocalTrainAcc=0.7838\n",
      "[Rank 2] Epoch 1/10 LocalTrainAcc=0.7857\n",
      "   >>> [Mode=FSDP_AMP] Epoch 1/10 | TrainAcc=0.7828 ValAcc=0.8411 | EpochTime=97.62s | PeakMem=0.46 GB\n",
      "[Rank 0] Epoch 2/10 LocalTrainAcc=0.8375\n",
      "[Rank 1] Epoch 2/10 LocalTrainAcc=0.8385\n",
      "[Rank 3] Epoch 2/10 LocalTrainAcc=0.8364\n",
      "[Rank 2] Epoch 2/10 LocalTrainAcc=0.8337\n",
      "   >>> [Mode=FSDP_AMP] Epoch 2/10 | TrainAcc=0.8365 ValAcc=0.8561 | EpochTime=96.33s | PeakMem=0.46 GB\n",
      "[Rank 1] Epoch 3/10 LocalTrainAcc=0.8520\n",
      "[Rank 0] Epoch 3/10 LocalTrainAcc=0.8535\n",
      "[Rank 3] Epoch 3/10 LocalTrainAcc=0.8515\n",
      "[Rank 2] Epoch 3/10 LocalTrainAcc=0.8506\n",
      "   >>> [Mode=FSDP_AMP] Epoch 3/10 | TrainAcc=0.8519 ValAcc=0.8578 | EpochTime=94.80s | PeakMem=0.46 GB\n",
      "[Rank 1] Epoch 4/10 LocalTrainAcc=0.8550\n",
      "[Rank 0] Epoch 4/10 LocalTrainAcc=0.8530\n",
      "[Rank 2] Epoch 4/10 LocalTrainAcc=0.8556\n",
      "[Rank 3] Epoch 4/10 LocalTrainAcc=0.8557\n",
      "   >>> [Mode=FSDP_AMP] Epoch 4/10 | TrainAcc=0.8548 ValAcc=0.8513 | EpochTime=95.37s | PeakMem=0.46 GB\n",
      "[Rank 2] Epoch 5/10 LocalTrainAcc=0.8686\n",
      "[Rank 1] Epoch 5/10 LocalTrainAcc=0.8640\n",
      "[Rank 0] Epoch 5/10 LocalTrainAcc=0.8665\n",
      "[Rank 3] Epoch 5/10 LocalTrainAcc=0.8664\n",
      "   >>> [Mode=FSDP_AMP] Epoch 5/10 | TrainAcc=0.8664 ValAcc=0.8772 | EpochTime=95.98s | PeakMem=0.46 GB\n",
      "[Rank 2] Epoch 6/10 LocalTrainAcc=0.8739\n",
      "[Rank 1] Epoch 6/10 LocalTrainAcc=0.8712\n",
      "[Rank 3] Epoch 6/10 LocalTrainAcc=0.8738\n",
      "[Rank 0] Epoch 6/10 LocalTrainAcc=0.8745\n",
      "   >>> [Mode=FSDP_AMP] Epoch 6/10 | TrainAcc=0.8733 ValAcc=0.8546 | EpochTime=96.40s | PeakMem=0.46 GB\n",
      "[Rank 0] Epoch 7/10 LocalTrainAcc=0.8840\n",
      "[Rank 1] Epoch 7/10 LocalTrainAcc=0.8817\n",
      "[Rank 3] Epoch 7/10 LocalTrainAcc=0.8829\n",
      "[Rank 2] Epoch 7/10 LocalTrainAcc=0.8860\n",
      "   >>> [Mode=FSDP_AMP] Epoch 7/10 | TrainAcc=0.8836 ValAcc=0.9073 | EpochTime=96.82s | PeakMem=0.46 GB\n",
      "[Rank 1] Epoch 8/10 LocalTrainAcc=0.5592\n",
      "[Rank 3] Epoch 8/10 LocalTrainAcc=0.5612\n",
      "[Rank 2] Epoch 8/10 LocalTrainAcc=0.5612\n",
      "[Rank 0] Epoch 8/10 LocalTrainAcc=0.5579\n",
      "   >>> [Mode=FSDP_AMP] Epoch 8/10 | TrainAcc=0.5599 ValAcc=0.5000 | EpochTime=96.65s | PeakMem=0.46 GB\n",
      "[Rank 2] Epoch 9/10 LocalTrainAcc=0.5032\n",
      "[Rank 0] Epoch 9/10 LocalTrainAcc=0.4989\n",
      "[Rank 3] Epoch 9/10 LocalTrainAcc=0.4998\n",
      "[Rank 1] Epoch 9/10 LocalTrainAcc=0.4982\n",
      "   >>> [Mode=FSDP_AMP] Epoch 9/10 | TrainAcc=0.5000 ValAcc=0.5000 | EpochTime=96.37s | PeakMem=0.46 GB\n",
      "[Rank 0] Epoch 10/10 LocalTrainAcc=0.5000\n",
      "[Rank 1] Epoch 10/10 LocalTrainAcc=0.5037\n",
      "[Rank 2] Epoch 10/10 LocalTrainAcc=0.4966\n",
      "[Rank 3] Epoch 10/10 LocalTrainAcc=0.4996\n",
      "   >>> [Mode=FSDP_AMP] Epoch 10/10 | TrainAcc=0.5000 ValAcc=0.5000 | EpochTime=96.39s | PeakMem=0.46 GB\n",
      "\n",
      "[SUMMARY][Mode=FSDP_AMP] [GPUs=4] [Batch=64] Total time for 10 epochs (including resume): 1137.49s\n",
      "\n",
      "*** Completed Mode=FSDP_AMP, GPUs=4, Batch=64 in 1154.64s ***\n",
      "\n",
      "=== Running Mode=FSDP_AMP, GPUs=4, BatchSize=128 ===\n",
      "[START] Mode=fsdp_amp, GPUs=4, Batch=128, 10 epochs\n",
      "[Rank 0] Epoch 1/10 LocalTrainAcc=0.5838\n",
      "[Rank 1] Epoch 1/10 LocalTrainAcc=0.5879\n",
      "[Rank 2] Epoch 1/10 LocalTrainAcc=0.5840\n",
      "[Rank 3] Epoch 1/10 LocalTrainAcc=0.5881\n",
      "   >>> [Mode=FSDP_AMP] Epoch 1/10 | TrainAcc=0.5859 ValAcc=0.5000 | EpochTime=87.61s | PeakMem=0.85 GB\n",
      "[Rank 2] Epoch 2/10 LocalTrainAcc=0.4996\n",
      "[Rank 0] Epoch 2/10 LocalTrainAcc=0.5017\n",
      "[Rank 1] Epoch 2/10 LocalTrainAcc=0.4964\n",
      "[Rank 3] Epoch 2/10 LocalTrainAcc=0.5023\n",
      "   >>> [Mode=FSDP_AMP] Epoch 2/10 | TrainAcc=0.5000 ValAcc=0.5000 | EpochTime=87.34s | PeakMem=0.85 GB\n",
      "[Rank 0] Epoch 3/10 LocalTrainAcc=0.5036\n",
      "[Rank 2] Epoch 3/10 LocalTrainAcc=0.4979\n",
      "[Rank 3] Epoch 3/10 LocalTrainAcc=0.4990\n",
      "[Rank 1] Epoch 3/10 LocalTrainAcc=0.4996\n",
      "   >>> [Mode=FSDP_AMP] Epoch 3/10 | TrainAcc=0.5000 ValAcc=0.5000 | EpochTime=86.97s | PeakMem=0.85 GB\n",
      "[Rank 2] Epoch 4/10 LocalTrainAcc=0.4993\n",
      "[Rank 0] Epoch 4/10 LocalTrainAcc=0.4987\n",
      "[Rank 1] Epoch 4/10 LocalTrainAcc=0.5042\n",
      "[Rank 3] Epoch 4/10 LocalTrainAcc=0.4978\n",
      "   >>> [Mode=FSDP_AMP] Epoch 4/10 | TrainAcc=0.5000 ValAcc=0.5000 | EpochTime=87.71s | PeakMem=0.85 GB\n",
      "[Rank 2] Epoch 5/10 LocalTrainAcc=0.5014\n",
      "[Rank 1] Epoch 5/10 LocalTrainAcc=0.4970\n",
      "[Rank 3] Epoch 5/10 LocalTrainAcc=0.5003\n",
      "[Rank 0] Epoch 5/10 LocalTrainAcc=0.5014\n",
      "   >>> [Mode=FSDP_AMP] Epoch 5/10 | TrainAcc=0.5000 ValAcc=0.5000 | EpochTime=87.27s | PeakMem=0.85 GB\n",
      "[Rank 2] Epoch 6/10 LocalTrainAcc=0.5001\n",
      "[Rank 0] Epoch 6/10 LocalTrainAcc=0.4987\n",
      "[Rank 1] Epoch 6/10 LocalTrainAcc=0.5007\n",
      "[Rank 3] Epoch 6/10 LocalTrainAcc=0.5004\n",
      "   >>> [Mode=FSDP_AMP] Epoch 6/10 | TrainAcc=0.5000 ValAcc=0.5000 | EpochTime=86.56s | PeakMem=0.85 GB\n",
      "[Rank 0] Epoch 7/10 LocalTrainAcc=0.5000\n",
      "[Rank 2] Epoch 7/10 LocalTrainAcc=0.5007\n",
      "[Rank 3] Epoch 7/10 LocalTrainAcc=0.4986\n",
      "[Rank 1] Epoch 7/10 LocalTrainAcc=0.5007\n",
      "   >>> [Mode=FSDP_AMP] Epoch 7/10 | TrainAcc=0.5000 ValAcc=0.5000 | EpochTime=85.48s | PeakMem=0.85 GB\n",
      "[Rank 0] Epoch 8/10 LocalTrainAcc=0.4985\n",
      "[Rank 1] Epoch 8/10 LocalTrainAcc=0.5003\n",
      "[Rank 2] Epoch 8/10 LocalTrainAcc=0.5005\n",
      "[Rank 3] Epoch 8/10 LocalTrainAcc=0.5007\n",
      "   >>> [Mode=FSDP_AMP] Epoch 8/10 | TrainAcc=0.5000 ValAcc=0.5000 | EpochTime=85.11s | PeakMem=0.85 GB\n",
      "[Rank 0] Epoch 9/10 LocalTrainAcc=0.4989\n",
      "[Rank 1] Epoch 9/10 LocalTrainAcc=0.4982\n",
      "[Rank 2] Epoch 9/10 LocalTrainAcc=0.5032\n",
      "[Rank 3] Epoch 9/10 LocalTrainAcc=0.4998\n",
      "   >>> [Mode=FSDP_AMP] Epoch 9/10 | TrainAcc=0.5000 ValAcc=0.5000 | EpochTime=85.32s | PeakMem=0.85 GB\n",
      "[Rank 1] Epoch 10/10 LocalTrainAcc=0.5037\n",
      "[Rank 2] Epoch 10/10 LocalTrainAcc=0.4966\n",
      "[Rank 3] Epoch 10/10 LocalTrainAcc=0.4996\n",
      "[Rank 0] Epoch 10/10 LocalTrainAcc=0.5000\n",
      "   >>> [Mode=FSDP_AMP] Epoch 10/10 | TrainAcc=0.5000 ValAcc=0.5000 | EpochTime=85.44s | PeakMem=0.85 GB\n",
      "\n",
      "[SUMMARY][Mode=FSDP_AMP] [GPUs=4] [Batch=128] Total time for 10 epochs (including resume): 1029.44s\n",
      "\n",
      "*** Completed Mode=FSDP_AMP, GPUs=4, Batch=128 in 1046.69s ***\n",
      "\n",
      "=== Running Mode=FSDP_AMP, GPUs=4, BatchSize=512 ===\n",
      "[START] Mode=fsdp_amp, GPUs=4, Batch=512, 10 epochs\n",
      "[Rank 0] Epoch 1/10 LocalTrainAcc=0.7020\n",
      "[Rank 1] Epoch 1/10 LocalTrainAcc=0.7029\n",
      "[Rank 2] Epoch 1/10 LocalTrainAcc=0.7028\n",
      "[Rank 3] Epoch 1/10 LocalTrainAcc=0.7064\n",
      "   >>> [Mode=FSDP_AMP] Epoch 1/10 | TrainAcc=0.7035 ValAcc=0.7867 | EpochTime=85.22s | PeakMem=3.21 GB\n",
      "[Rank 1] Epoch 2/10 LocalTrainAcc=0.6682\n",
      "[Rank 0] Epoch 2/10 LocalTrainAcc=0.6649\n",
      "[Rank 2] Epoch 2/10 LocalTrainAcc=0.6672\n",
      "[Rank 3] Epoch 2/10 LocalTrainAcc=0.6690\n",
      "   >>> [Mode=FSDP_AMP] Epoch 2/10 | TrainAcc=0.6673 ValAcc=0.5000 | EpochTime=81.10s | PeakMem=3.21 GB\n",
      "[Rank 1] Epoch 3/10 LocalTrainAcc=0.4996\n",
      "[Rank 0] Epoch 3/10 LocalTrainAcc=0.5036\n",
      "[Rank 2] Epoch 3/10 LocalTrainAcc=0.4979\n",
      "[Rank 3] Epoch 3/10 LocalTrainAcc=0.4990\n",
      "   >>> [Mode=FSDP_AMP] Epoch 3/10 | TrainAcc=0.5000 ValAcc=0.5000 | EpochTime=81.24s | PeakMem=3.21 GB\n",
      "[Rank 1] Epoch 4/10 LocalTrainAcc=0.5042\n",
      "[Rank 0] Epoch 4/10 LocalTrainAcc=0.4987\n",
      "[Rank 2] Epoch 4/10 LocalTrainAcc=0.4993\n",
      "[Rank 3] Epoch 4/10 LocalTrainAcc=0.4978\n",
      "   >>> [Mode=FSDP_AMP] Epoch 4/10 | TrainAcc=0.5000 ValAcc=0.5000 | EpochTime=81.52s | PeakMem=3.21 GB\n",
      "[Rank 0] Epoch 5/10 LocalTrainAcc=0.5014\n",
      "[Rank 2] Epoch 5/10 LocalTrainAcc=0.5014\n",
      "[Rank 1] Epoch 5/10 LocalTrainAcc=0.4970\n",
      "[Rank 3] Epoch 5/10 LocalTrainAcc=0.5003\n",
      "   >>> [Mode=FSDP_AMP] Epoch 5/10 | TrainAcc=0.5000 ValAcc=0.5000 | EpochTime=81.40s | PeakMem=3.21 GB\n",
      "[Rank 2] Epoch 6/10 LocalTrainAcc=0.5001\n",
      "[Rank 1] Epoch 6/10 LocalTrainAcc=0.5007\n",
      "[Rank 3] Epoch 6/10 LocalTrainAcc=0.5004\n",
      "[Rank 0] Epoch 6/10 LocalTrainAcc=0.4987\n",
      "   >>> [Mode=FSDP_AMP] Epoch 6/10 | TrainAcc=0.5000 ValAcc=0.5000 | EpochTime=80.97s | PeakMem=3.21 GB\n",
      "[Rank 0] Epoch 7/10 LocalTrainAcc=0.5000\n",
      "[Rank 1] Epoch 7/10 LocalTrainAcc=0.5007\n",
      "[Rank 3] Epoch 7/10 LocalTrainAcc=0.4986\n",
      "[Rank 2] Epoch 7/10 LocalTrainAcc=0.5007\n",
      "   >>> [Mode=FSDP_AMP] Epoch 7/10 | TrainAcc=0.5000 ValAcc=0.5000 | EpochTime=81.26s | PeakMem=3.21 GB\n",
      "[Rank 0] Epoch 8/10 LocalTrainAcc=0.4985\n",
      "[Rank 2] Epoch 8/10 LocalTrainAcc=0.5005\n",
      "[Rank 1] Epoch 8/10 LocalTrainAcc=0.5003\n",
      "[Rank 3] Epoch 8/10 LocalTrainAcc=0.5007\n",
      "   >>> [Mode=FSDP_AMP] Epoch 8/10 | TrainAcc=0.5000 ValAcc=0.5000 | EpochTime=81.29s | PeakMem=3.21 GB\n",
      "[Rank 1] Epoch 9/10 LocalTrainAcc=0.4982\n",
      "[Rank 2] Epoch 9/10 LocalTrainAcc=0.5032\n",
      "[Rank 0] Epoch 9/10 LocalTrainAcc=0.4989\n",
      "[Rank 3] Epoch 9/10 LocalTrainAcc=0.4998\n",
      "   >>> [Mode=FSDP_AMP] Epoch 9/10 | TrainAcc=0.5000 ValAcc=0.5000 | EpochTime=80.87s | PeakMem=3.21 GB\n",
      "[Rank 0] Epoch 10/10 LocalTrainAcc=0.5000\n",
      "[Rank 2] Epoch 10/10 LocalTrainAcc=0.4966\n",
      "[Rank 1] Epoch 10/10 LocalTrainAcc=0.5037\n",
      "[Rank 3] Epoch 10/10 LocalTrainAcc=0.4996\n",
      "   >>> [Mode=FSDP_AMP] Epoch 10/10 | TrainAcc=0.5000 ValAcc=0.5000 | EpochTime=81.79s | PeakMem=3.21 GB\n",
      "\n",
      "[SUMMARY][Mode=FSDP_AMP] [GPUs=4] [Batch=512] Total time for 10 epochs (including resume): 979.49s\n",
      "\n",
      "*** Completed Mode=FSDP_AMP, GPUs=4, Batch=512 in 996.64s ***\n",
      "\n",
      "FSDP benchmark completed. Metrics saved in 'fsdp_benchmark_metrics.csv'.\n"
     ]
    }
   ],
   "source": [
    "!python fsdp_benchmark_with_metrics_report.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc14abf-8ab9-481a-b018-283a642c3656",
   "metadata": {},
   "source": [
    "## Re-run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6db33c2-d5fe-47f0-bbbc-a748b4c5d679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting clean_fsdp_amp_2_4.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile clean_fsdp_amp_2_4.py\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "CSV_PATH = \"fsdp_benchmark_metrics.csv\"\n",
    "CKPT_DIR = \"checkpoints\"\n",
    "\n",
    "# --- 1. Clean the CSV -------------------------------------------------\n",
    "if os.path.exists(CSV_PATH):\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    print(f\"Loaded {len(df)} rows from {CSV_PATH}\")\n",
    "\n",
    "    # Rows we want to drop: fsdp_amp with gpu_count 2 or 4\n",
    "    drop_mask = (df[\"mode\"] == \"fsdp_amp\") & (df[\"gpu_count\"].isin([1, 2, 4]))\n",
    "    n_drop = drop_mask.sum()\n",
    "\n",
    "    df_clean = df[~drop_mask].reset_index(drop=True)\n",
    "    df_clean.to_csv(CSV_PATH, index=False)\n",
    "\n",
    "    print(f\"Removed {n_drop} rows where mode='fsdp_amp' and gpu_count in [2, 4].\")\n",
    "    print(f\"New CSV row count: {len(df_clean)}\")\n",
    "else:\n",
    "    print(f\"{CSV_PATH} not found. Skipping CSV cleanup.\")\n",
    "\n",
    "# --- 2. Delete old checkpoints for fsdp_amp, GPUs 2 and 4 -------------\n",
    "# In the FSDP script we named checkpoints:\n",
    "#   ckpt_name = f\\\"fsdp_{mode}_g{world_size}_b{batch_size}.pt\\\"\n",
    "# So for mode='fsdp_amp' and world_size=2, batch=64:\n",
    "#   'fsdp_fsdp_amp_g2_b64.pt'\n",
    "\n",
    "if os.path.isdir(CKPT_DIR):\n",
    "    modes = [\"fsdp_amp\"]\n",
    "    gpu_counts = [1, 2, 4]\n",
    "    batch_sizes = [64, 128, 512]\n",
    "\n",
    "    removed = 0\n",
    "    for mode in modes:\n",
    "        for g in gpu_counts:\n",
    "            for b in batch_sizes:\n",
    "                ckpt_name = f\"fsdp_{mode}_g{g}_b{b}.pt\"\n",
    "                ckpt_path = os.path.join(CKPT_DIR, ckpt_name)\n",
    "                if os.path.exists(ckpt_path):\n",
    "                    os.remove(ckpt_path)\n",
    "                    removed += 1\n",
    "                    print(f\"Removed checkpoint: {ckpt_path}\")\n",
    "\n",
    "    if removed == 0:\n",
    "        print(\"No matching fsdp_amp checkpoints found to remove.\")\n",
    "    else:\n",
    "        print(f\"Total checkpoints removed: {removed}\")\n",
    "else:\n",
    "    print(f\"No '{CKPT_DIR}' directory found. Skipping checkpoint cleanup.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d117b8e6-1135-4b7b-8d0a-bd060097c0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 120 rows from fsdp_benchmark_metrics.csv\n",
      "Removed 30 rows where mode='fsdp_amp' and gpu_count in [2, 4].\n",
      "New CSV row count: 90\n",
      "No matching fsdp_amp checkpoints found to remove.\n"
     ]
    }
   ],
   "source": [
    "!python clean_fsdp_amp_2_4.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fce4d91-2822-4aa3-8228-3faeb42dc7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting fsdp_fsdp_amp_rerun_g2_g4_metrics2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile fsdp_fsdp_amp_rerun_g2_g4_metrics2.py\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "from PIL import Image\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.multiprocessing as mp\n",
    "import torch.distributed as dist\n",
    "\n",
    "from torch.nn import Module\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.distributed.fsdp import FullyShardedDataParallel as FSDP, ShardingStrategy\n",
    "\n",
    "# AMP check\n",
    "try:\n",
    "    from torch.amp import autocast, GradScaler\n",
    "    USE_NEW_AMP = True\n",
    "except ImportError:\n",
    "    from torch.cuda.amp import autocast, GradScaler\n",
    "    USE_NEW_AMP = False\n",
    "\n",
    "# ============================================================\n",
    "# 1. DISTRIBUTED SETUP / CLEANUP\n",
    "# ============================================================\n",
    "def setup_dist(rank, world_size):\n",
    "    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "    os.environ[\"MASTER_PORT\"] = \"12387\"  \n",
    "    dist.init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n",
    "    torch.cuda.set_device(rank)\n",
    "\n",
    "def cleanup_dist():\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "# ============================================================\n",
    "# 2. DATASET & MODEL\n",
    "# ============================================================\n",
    "class OralCancerDataset(Dataset):\n",
    "    def __init__(self, dataframe, path_map, transform=None):\n",
    "        self.data = dataframe.reset_index(drop=True)\n",
    "        self.path_map = path_map\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        img_id = row[\"id\"]\n",
    "        label = int(row[\"label\"])\n",
    "        img_path = self.path_map.get(img_id)\n",
    "        if img_path is None:\n",
    "            image = Image.new(\"RGB\", (96, 96))\n",
    "        else:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "class SimpleCNN(Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(128 * 12 * 12, 256)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = x.view(-1, 128 * 12 * 12)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# ============================================================\n",
    "# 3. WORKER: FSDP + AMP TRAINING FOR ONE CONFIG\n",
    "# ============================================================\n",
    "def fsdp_amp_worker(rank, world_size, df, path_map, batch_size, num_epochs, csv_path):\n",
    "    setup_dist(rank, world_size)\n",
    "    device = torch.device(f\"cuda:{rank}\")\n",
    "\n",
    "    IMG_SIZE = 96\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5]*3, [0.5]*3),\n",
    "    ])\n",
    "    val_tf = transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5]*3, [0.5]*3),\n",
    "    ])\n",
    "\n",
    "    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df[\"label\"])\n",
    "\n",
    "    train_dataset = OralCancerDataset(train_df, path_map, transform=train_tf)\n",
    "    val_dataset   = OralCancerDataset(val_df,   path_map, transform=val_tf)\n",
    "\n",
    "    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank, shuffle=True)\n",
    "    val_sampler = DistributedSampler(val_dataset, num_replicas=world_size, rank=rank, shuffle=False)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler, shuffle=False, num_workers=0, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, sampler=val_sampler, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "    model = SimpleCNN().to(device)\n",
    "    sharding_strategy = ShardingStrategy.NO_SHARD if world_size == 1 else ShardingStrategy.FULL_SHARD\n",
    "    model = FSDP(model, device_id=device, sharding_strategy=sharding_strategy)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    # FIX 1: LOWER LEARNING RATE\n",
    "    # 1e-3 is often too unstable for Multi-GPU AMP. 1e-4 is safer.\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4) \n",
    "    \n",
    "    scaler = GradScaler()\n",
    "\n",
    "    if rank == 0:\n",
    "        print(f\"[FSDP_AMP] world_size={world_size}, batch_size={batch_size}\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_sampler.set_epoch(epoch)\n",
    "        torch.cuda.reset_peak_memory_stats(device)\n",
    "        epoch_start = time.time()\n",
    "\n",
    "        # ----- TRAIN -----\n",
    "        model.train()\n",
    "        train_loss_sum = 0.0\n",
    "        train_correct_sum = 0\n",
    "        train_total = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            labels = labels.float().unsqueeze(1).to(device, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if USE_NEW_AMP:\n",
    "                amp_ctx = autocast(device_type=\"cuda\")\n",
    "            else:\n",
    "                amp_ctx = autocast()\n",
    "\n",
    "            with amp_ctx:\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            # FIX 2: GRADIENT CLIPPING WITH AMP\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            # Unscale the optimizer first\n",
    "            scaler.unscale_(optimizer)\n",
    "            # Now clip gradients to prevent them from hitting Infinity\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            # FIX 3: NAN CHECK BEFORE SUMMING\n",
    "            loss_val = loss.item()\n",
    "            if not math.isnan(loss_val) and not math.isinf(loss_val):\n",
    "                train_loss_sum += loss_val * images.size(0)\n",
    "            \n",
    "            probs = torch.sigmoid(outputs)\n",
    "            preds = (probs >= 0.5).float()\n",
    "            train_correct_sum += (preds == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "\n",
    "        epoch_time = time.time() - epoch_start\n",
    "\n",
    "        # ----- VAL -----\n",
    "        model.eval()\n",
    "        val_loss_sum = 0.0\n",
    "        val_correct_sum = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images = images.to(device, non_blocking=True)\n",
    "                labels = labels.float().unsqueeze(1).to(device, non_blocking=True)\n",
    "\n",
    "                if USE_NEW_AMP:\n",
    "                    amp_ctx = autocast(device_type=\"cuda\")\n",
    "                else:\n",
    "                    amp_ctx = autocast()\n",
    "\n",
    "                with amp_ctx:\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                # FIX 3 (Applied to Val): NAN CHECK\n",
    "                loss_val = loss.item()\n",
    "                if not math.isnan(loss_val) and not math.isinf(loss_val):\n",
    "                    val_loss_sum += loss_val * images.size(0)\n",
    "\n",
    "                probs = torch.sigmoid(outputs)\n",
    "                preds = (probs >= 0.5).float()\n",
    "                val_correct_sum += (preds == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "        # ----- METRICS -----\n",
    "        metrics = torch.tensor([\n",
    "            train_loss_sum, train_correct_sum, train_total,\n",
    "            val_loss_sum, val_correct_sum, val_total,\n",
    "        ], device=device)\n",
    "        dist.all_reduce(metrics, op=dist.ReduceOp.SUM)\n",
    "\n",
    "        peak_mem_local = torch.cuda.max_memory_allocated(device)\n",
    "        mem_tensor = torch.tensor(peak_mem_local, device=device, dtype=torch.float64)\n",
    "        dist.all_reduce(mem_tensor, op=dist.ReduceOp.MAX)\n",
    "        peak_mem_bytes = mem_tensor.item()\n",
    "        peak_mem_gb = peak_mem_bytes / (1024**3)\n",
    "\n",
    "        g_train_loss = metrics[0].item() / max(metrics[2].item(), 1)\n",
    "        g_train_acc  = metrics[1].item() / max(metrics[2].item(), 1)\n",
    "        g_val_loss   = metrics[3].item() / max(metrics[5].item(), 1)\n",
    "        g_val_acc    = metrics[4].item() / max(metrics[5].item(), 1)\n",
    "\n",
    "        if rank == 0:\n",
    "            print(f\" >>> [FSDP_AMP] GPUs={world_size}, epoch={epoch+1} | TrainLoss={g_train_loss:.4f} ValAcc={g_val_acc:.4f}\")\n",
    "            row = {\n",
    "                \"mode\": \"fsdp_amp\",\n",
    "                \"gpu_count\": world_size,\n",
    "                \"batch_size\": batch_size,\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"train_loss\": g_train_loss,\n",
    "                \"train_acc\": g_train_acc,\n",
    "                \"val_loss\": g_val_loss,\n",
    "                \"val_acc\": g_val_acc,\n",
    "                \"epoch_time\": epoch_time,\n",
    "                \"peak_mem_bytes\": peak_mem_bytes,\n",
    "                \"peak_mem_gb\": peak_mem_gb,\n",
    "            }\n",
    "            file_exists = os.path.exists(csv_path)\n",
    "            pd.DataFrame([row]).to_csv(csv_path, mode=\"a\", header=not file_exists, index=False)\n",
    "\n",
    "    cleanup_dist()\n",
    "\n",
    "# ============================================================\n",
    "# 4. MAIN\n",
    "# ============================================================\n",
    "if __name__ == \"__main__\":\n",
    "    mp.set_start_method(\"spawn\", force=True)\n",
    "    \n",
    "    # We save to the SAME file you used before, so it just appends the missing data\n",
    "    CSV_PATH = \"fsdp_benchmark_metrics2.csv\"\n",
    "    \n",
    "    print(\"Loading dataset...\")\n",
    "    df = pd.read_csv(\"oral_cancer_balanced.csv\")\n",
    "    path_map = {}\n",
    "    for root, dirs, files in os.walk(\"Data\"):\n",
    "        if \"val\" in dirs: dirs.remove(\"val\")\n",
    "        for file in files:\n",
    "            if file.lower().endswith((\".jpg\", \".jpeg\", \".png\")) and \"val\" not in root:\n",
    "                path_map[file] = os.path.join(root, file)\n",
    "\n",
    "    max_gpus = torch.cuda.device_count()\n",
    "    \n",
    "    # ONLY RERUN 2 and 4, since 1 worked\n",
    "    target_gpu_counts = [2, 4]\n",
    "    gpu_counts = [g for g in target_gpu_counts if g <= max_gpus]\n",
    "\n",
    "    batch_sizes = [64, 128, 512] \n",
    "    num_epochs = 10 \n",
    "\n",
    "    for n_gpus in gpu_counts:\n",
    "        for b_size in batch_sizes:\n",
    "            print(f\"\\n=== RUN: GPUs={n_gpus}, batch_size={b_size} ===\")\n",
    "            try:\n",
    "                mp.spawn(fsdp_amp_worker, args=(n_gpus, df, path_map, b_size, num_epochs, CSV_PATH), nprocs=n_gpus, join=True)\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] GPUs={n_gpus}, batch={b_size} :: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c69f3ea-17d2-4de2-9fde-4eeb1feb3d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "\n",
      "=== RUN: GPUs=2, batch_size=64 ===\n",
      "[FSDP_AMP] world_size=2, batch_size=64\n",
      " >>> [FSDP_AMP] GPUs=2, epoch=1 | TrainLoss=0.1090 ValAcc=0.5000\n",
      " >>> [FSDP_AMP] GPUs=2, epoch=2 | TrainLoss=0.0000 ValAcc=0.5000\n",
      " >>> [FSDP_AMP] GPUs=2, epoch=4 | TrainLoss=0.0000 ValAcc=0.5000\n",
      " >>> [FSDP_AMP] GPUs=2, epoch=6 | TrainLoss=0.0000 ValAcc=0.5000\n",
      " >>> [FSDP_AMP] GPUs=2, epoch=7 | TrainLoss=0.0000 ValAcc=0.5000\n",
      " >>> [FSDP_AMP] GPUs=2, epoch=9 | TrainLoss=0.0000 ValAcc=0.5000\n",
      " >>> [FSDP_AMP] GPUs=2, epoch=10 | TrainLoss=0.0000 ValAcc=0.5000\n",
      "\n",
      "=== RUN: GPUs=2, batch_size=128 ===\n",
      "[FSDP_AMP] world_size=2, batch_size=128\n",
      " >>> [FSDP_AMP] GPUs=2, epoch=1 | TrainLoss=0.2041 ValAcc=0.5000\n",
      " >>> [FSDP_AMP] GPUs=2, epoch=3 | TrainLoss=0.0000 ValAcc=0.5000\n",
      " >>> [FSDP_AMP] GPUs=2, epoch=4 | TrainLoss=0.0000 ValAcc=0.5000\n",
      " >>> [FSDP_AMP] GPUs=2, epoch=5 | TrainLoss=0.0000 ValAcc=0.5000\n",
      " >>> [FSDP_AMP] GPUs=2, epoch=6 | TrainLoss=0.0000 ValAcc=0.5000\n",
      " >>> [FSDP_AMP] GPUs=2, epoch=7 | TrainLoss=0.0000 ValAcc=0.5000\n",
      " >>> [FSDP_AMP] GPUs=2, epoch=9 | TrainLoss=0.0000 ValAcc=0.5000\n",
      " >>> [FSDP_AMP] GPUs=2, epoch=10 | TrainLoss=0.0000 ValAcc=0.5000\n",
      "\n",
      "=== RUN: GPUs=2, batch_size=512 ===\n",
      "[FSDP_AMP] world_size=2, batch_size=512\n",
      " >>> [FSDP_AMP] GPUs=2, epoch=1 | TrainLoss=0.4430 ValAcc=0.8126\n",
      " >>> [FSDP_AMP] GPUs=2, epoch=2 | TrainLoss=0.3701 ValAcc=0.8381\n",
      " >>> [FSDP_AMP] GPUs=2, epoch=4 | TrainLoss=0.3324 ValAcc=0.8612\n",
      " >>> [FSDP_AMP] GPUs=2, epoch=5 | TrainLoss=0.3221 ValAcc=0.8637\n",
      " >>> [FSDP_AMP] GPUs=2, epoch=7 | TrainLoss=0.3032 ValAcc=0.8779\n",
      " >>> [FSDP_AMP] GPUs=2, epoch=8 | TrainLoss=0.2965 ValAcc=0.8676\n",
      " >>> [FSDP_AMP] GPUs=2, epoch=9 | TrainLoss=0.2906 ValAcc=0.8808\n",
      "[FSDP_AMP] world_size=4, batch_size=64\n",
      " >>> [FSDP_AMP] GPUs=4, epoch=1 | TrainLoss=0.1068 ValAcc=0.5000\n",
      " >>> [FSDP_AMP] GPUs=4, epoch=2 | TrainLoss=0.0000 ValAcc=0.5000\n",
      " >>> [FSDP_AMP] GPUs=4, epoch=3 | TrainLoss=0.0000 ValAcc=0.5000\n",
      " >>> [FSDP_AMP] GPUs=4, epoch=4 | TrainLoss=0.0000 ValAcc=0.5000\n",
      " >>> [FSDP_AMP] GPUs=4, epoch=5 | TrainLoss=0.0000 ValAcc=0.5000\n",
      " >>> [FSDP_AMP] GPUs=4, epoch=6 | TrainLoss=0.0000 ValAcc=0.5000\n",
      " >>> [FSDP_AMP] GPUs=4, epoch=7 | TrainLoss=0.0000 ValAcc=0.5000\n",
      " >>> [FSDP_AMP] GPUs=4, epoch=9 | TrainLoss=0.0000 ValAcc=0.5000\n",
      " >>> [FSDP_AMP] GPUs=4, epoch=10 | TrainLoss=0.0000 ValAcc=0.5000\n",
      "\n",
      "=== RUN: GPUs=4, batch_size=128 ===\n",
      "[FSDP_AMP] world_size=4, batch_size=128\n",
      " >>> [FSDP_AMP] GPUs=4, epoch=1 | TrainLoss=0.4318 ValAcc=0.8342\n",
      " >>> [FSDP_AMP] GPUs=4, epoch=2 | TrainLoss=0.3723 ValAcc=0.8053\n",
      " >>> [FSDP_AMP] GPUs=4, epoch=3 | TrainLoss=0.3006 ValAcc=0.5000\n",
      " >>> [FSDP_AMP] GPUs=4, epoch=4 | TrainLoss=0.0000 ValAcc=0.5000\n",
      " >>> [FSDP_AMP] GPUs=4, epoch=5 | TrainLoss=0.0000 ValAcc=0.5000\n",
      " >>> [FSDP_AMP] GPUs=4, epoch=6 | TrainLoss=0.0000 ValAcc=0.5000\n",
      " >>> [FSDP_AMP] GPUs=4, epoch=7 | TrainLoss=0.0000 ValAcc=0.5000\n",
      " >>> [FSDP_AMP] GPUs=4, epoch=8 | TrainLoss=0.0000 ValAcc=0.5000\n",
      " >>> [FSDP_AMP] GPUs=4, epoch=9 | TrainLoss=0.0000 ValAcc=0.5000\n",
      " >>> [FSDP_AMP] GPUs=4, epoch=10 | TrainLoss=0.0000 ValAcc=0.5000\n",
      "\n",
      "=== RUN: GPUs=4, batch_size=512 ===\n",
      "[FSDP_AMP] world_size=4, batch_size=512\n",
      " >>> [FSDP_AMP] GPUs=4, epoch=2 | TrainLoss=0.3957 ValAcc=0.8288\n",
      " >>> [FSDP_AMP] GPUs=4, epoch=3 | TrainLoss=0.3685 ValAcc=0.8372\n",
      " >>> [FSDP_AMP] GPUs=4, epoch=4 | TrainLoss=0.3548 ValAcc=0.8494\n",
      " >>> [FSDP_AMP] GPUs=4, epoch=5 | TrainLoss=0.3445 ValAcc=0.8560\n",
      " >>> [FSDP_AMP] GPUs=4, epoch=6 | TrainLoss=0.3317 ValAcc=0.8541\n",
      " >>> [FSDP_AMP] GPUs=4, epoch=7 | TrainLoss=0.3269 ValAcc=0.8573\n",
      " >>> [FSDP_AMP] GPUs=4, epoch=8 | TrainLoss=0.3172 ValAcc=0.8682\n",
      " >>> [FSDP_AMP] GPUs=4, epoch=9 | TrainLoss=0.3114 ValAcc=0.8663\n",
      " >>> [FSDP_AMP] GPUs=4, epoch=10 | TrainLoss=0.3020 ValAcc=0.8568\n"
     ]
    }
   ],
   "source": [
    "!python fsdp_fsdp_amp_rerun_g2_g4_metrics2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8151f39d-13ac-4085-99d2-89677ea72e9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (hpc)",
   "language": "python",
   "name": "hpc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
