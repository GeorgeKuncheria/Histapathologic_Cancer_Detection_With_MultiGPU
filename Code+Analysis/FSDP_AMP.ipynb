{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d0eaa98-13b8-4d86-b74d-67991ac58f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: Tesla V100-SXM2-32GB (UUID: GPU-8e7bff07-76c8-52a0-2680-c1419e14718d)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9efefcd5-0a4e-4ff0-b8c8-cbaf7bd50992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec  9 22:06:59 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla V100-SXM2-32GB           On  | 00000000:86:00.0 Off |                    0 |\n",
      "| N/A   36C    P0              44W / 300W |      0MiB / 32768MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67cae45e-f9a1-4004-9247-d19b872ef09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting single_gpu_amp_benchmark.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile single_gpu_amp_benchmark.py\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.multiprocessing as mp\n",
    "import torch.distributed as dist\n",
    "\n",
    "from torch.nn import Module\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.distributed.fsdp import FullyShardedDataParallel as FSDP, ShardingStrategy\n",
    "\n",
    "# AMP Imports\n",
    "try:\n",
    "    from torch.amp import autocast, GradScaler\n",
    "    USE_NEW_AMP = True\n",
    "except ImportError:\n",
    "    from torch.cuda.amp import autocast, GradScaler\n",
    "    USE_NEW_AMP = False\n",
    "\n",
    "# ============================================================\n",
    "# 1. SETUP / CLEANUP\n",
    "# ============================================================\n",
    "\n",
    "def setup_dist(rank, world_size):\n",
    "    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "    os.environ[\"MASTER_PORT\"] = \"12356\" \n",
    "    dist.init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n",
    "    torch.cuda.set_device(rank)\n",
    "\n",
    "def cleanup_dist():\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "# ============================================================\n",
    "# 2. DATASET & MODEL\n",
    "# ============================================================\n",
    "\n",
    "class OralCancerDataset(Dataset):\n",
    "    def __init__(self, dataframe, path_map, transform=None):\n",
    "        self.data = dataframe.reset_index(drop=True)\n",
    "        self.path_map = path_map\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        img_id = row[\"id\"]\n",
    "        label = int(row[\"label\"])\n",
    "        \n",
    "        img_path = self.path_map.get(img_id)\n",
    "        if img_path is None:\n",
    "            image = Image.new(\"RGB\", (96, 96))\n",
    "        else:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label\n",
    "\n",
    "class SimpleCNN(Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(128 * 12 * 12, 256)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = x.view(-1, 128 * 12 * 12)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# ============================================================\n",
    "# 3. TRAINING WORKER (1 GPU + AMP + RESUME)\n",
    "# ============================================================\n",
    "\n",
    "def train_worker(rank, world_size, df, path_map, batch_size, num_epochs, csv_path):\n",
    "    setup_dist(rank, world_size)\n",
    "    device = torch.device(f\"cuda:{rank}\")\n",
    "    \n",
    "    # Transforms\n",
    "    IMG_SIZE = 96\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5]*3, [0.5]*3),\n",
    "    ])\n",
    "    val_tf = transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5]*3, [0.5]*3),\n",
    "    ])\n",
    "\n",
    "    # Split\n",
    "    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df[\"label\"])\n",
    "    \n",
    "    train_dataset = OralCancerDataset(train_df, path_map, transform=train_tf)\n",
    "    val_dataset   = OralCancerDataset(val_df,   path_map, transform=val_tf)\n",
    "\n",
    "    # Samplers\n",
    "    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank, shuffle=True)\n",
    "    val_sampler   = DistributedSampler(val_dataset, num_replicas=world_size, rank=rank, shuffle=False)\n",
    "\n",
    "    # Loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler, \n",
    "                              num_workers=0, pin_memory=True)\n",
    "    val_loader   = DataLoader(val_dataset, batch_size=batch_size, sampler=val_sampler, \n",
    "                              num_workers=0, pin_memory=True)\n",
    "\n",
    "    # Model Setup\n",
    "    model = SimpleCNN().to(device)\n",
    "    sharding_strategy = ShardingStrategy.NO_SHARD\n",
    "    model = FSDP(model, device_id=device, sharding_strategy=sharding_strategy)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    # --- CHECKPOINT & RESUME LOGIC ---\n",
    "    ckpt_dir = \"checkpoints_amp_fresh\"\n",
    "    os.makedirs(ckpt_dir, exist_ok=True)\n",
    "    ckpt_path = os.path.join(ckpt_dir, f\"1gpu_amp_batch{batch_size}.pt\")\n",
    "\n",
    "    start_epoch = 0\n",
    "    if os.path.exists(ckpt_path):\n",
    "        print(f\"[RESUME] Found checkpoint for Batch {batch_size}. Loading...\")\n",
    "        # Map location is crucial to ensure it loads to correct GPU\n",
    "        checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
    "        scaler.load_state_dict(checkpoint['scaler_state'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        print(f\"[RESUME] Resuming from Epoch {start_epoch}\")\n",
    "    else:\n",
    "        print(f\"[START] No checkpoint found. Starting fresh for Batch {batch_size}\")\n",
    "\n",
    "    # Start loop from start_epoch\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        train_sampler.set_epoch(epoch)\n",
    "        torch.cuda.reset_peak_memory_stats(device)\n",
    "        epoch_start = time.time()\n",
    "\n",
    "        # --- TRAIN ---\n",
    "        model.train()\n",
    "        train_loss_sum = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            labels = labels.float().unsqueeze(1).to(device, non_blocking=True)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if USE_NEW_AMP:\n",
    "                amp_ctx = autocast(device_type=\"cuda\")\n",
    "            else:\n",
    "                amp_ctx = autocast()\n",
    "                \n",
    "            with amp_ctx:\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            train_loss_sum += loss.item() * images.size(0)\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            preds = (probs >= 0.5).float()\n",
    "            train_correct += (preds == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "\n",
    "        epoch_time = time.time() - epoch_start\n",
    "\n",
    "        # --- VALIDATION ---\n",
    "        model.eval()\n",
    "        val_loss_sum = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images = images.to(device, non_blocking=True)\n",
    "                labels = labels.float().unsqueeze(1).to(device, non_blocking=True)\n",
    "                \n",
    "                if USE_NEW_AMP:\n",
    "                    amp_ctx = autocast(device_type=\"cuda\")\n",
    "                else:\n",
    "                    amp_ctx = autocast()\n",
    "                \n",
    "                with amp_ctx:\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss_sum += loss.item() * images.size(0)\n",
    "                probs = torch.sigmoid(outputs)\n",
    "                preds = (probs >= 0.5).float()\n",
    "                val_correct += (preds == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "        # Metrics\n",
    "        avg_train_loss = train_loss_sum / train_total\n",
    "        avg_train_acc  = train_correct / train_total\n",
    "        avg_val_loss   = val_loss_sum / val_total\n",
    "        avg_val_acc    = val_correct / val_total\n",
    "        \n",
    "        peak_mem = torch.cuda.max_memory_allocated(device)\n",
    "        peak_mem_gb = peak_mem / (1024**3)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Batch {batch_size} | \"\n",
    "              f\"TrainAcc={avg_train_acc:.4f} ValAcc={avg_val_acc:.4f} | \"\n",
    "              f\"Time={epoch_time:.2f}s Mem={peak_mem_gb:.2f}GB\")\n",
    "\n",
    "        # Save Checkpoint (Overwrite current epoch)\n",
    "        torch.save({\n",
    "            'epoch': epoch+1,\n",
    "            'model_state': model.state_dict(),\n",
    "            'optimizer_state': optimizer.state_dict(),\n",
    "            'scaler_state': scaler.state_dict()\n",
    "        }, ckpt_path)\n",
    "\n",
    "        # Log to CSV\n",
    "        row = {\n",
    "            \"mode\": \"fsdp_amp\",\n",
    "            \"gpu_count\": 1,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_loss\": avg_train_loss,\n",
    "            \"train_acc\": avg_train_acc,\n",
    "            \"val_loss\": avg_val_loss,\n",
    "            \"val_acc\": avg_val_acc,\n",
    "            \"epoch_time\": epoch_time,\n",
    "            \"peak_mem_bytes\": peak_mem,\n",
    "            \"peak_mem_gb\": peak_mem_gb\n",
    "        }\n",
    "        \n",
    "        file_exists = os.path.exists(csv_path)\n",
    "        pd.DataFrame([row]).to_csv(csv_path, mode=\"a\", header=not file_exists, index=False)\n",
    "\n",
    "    cleanup_dist()\n",
    "\n",
    "# ============================================================\n",
    "# 4. MAIN\n",
    "# ============================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mp.set_start_method(\"spawn\", force=True)\n",
    "    \n",
    "    CSV_FILE = \"FSDP_AMP_metrics.csv\"\n",
    "    \n",
    "    # 1. Load Data\n",
    "    print(\"Loading Dataset...\")\n",
    "    df = pd.read_csv(\"oral_cancer_balanced.csv\")\n",
    "    \n",
    "    path_map = {}\n",
    "    for root, dirs, files in os.walk(\"Data\"):\n",
    "        if \"val\" in dirs: dirs.remove(\"val\")\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                path_map[file] = os.path.join(root, file)\n",
    "                \n",
    "    # 2. Configurations\n",
    "    BATCH_SIZES = [64, 128, 512]\n",
    "    NUM_EPOCHS = 10\n",
    "    GPU_COUNT = 1\n",
    "    \n",
    "    print(f\"Starting Single GPU AMP Benchmark. Saving to {CSV_FILE}\")\n",
    "    \n",
    "    for bs in BATCH_SIZES:\n",
    "        print(f\"\\n=== Starting Run: Batch Size {bs} ===\")\n",
    "        try:\n",
    "            mp.spawn(\n",
    "                train_worker,\n",
    "                args=(GPU_COUNT, df, path_map, bs, NUM_EPOCHS, CSV_FILE),\n",
    "                nprocs=GPU_COUNT,\n",
    "                join=True\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error running batch size {bs}: {e}\")\n",
    "            \n",
    "    print(f\"\\nAll runs completed. Check {CSV_FILE} for results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea0515a8-f434-40aa-abea-bb62ab5a57d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaa47a30-60be-4e59-8e17-1538f4a12000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dataset...\n",
      "Starting Single GPU AMP Benchmark. Saving to FSDP_AMP_metrics.csv\n",
      "\n",
      "=== Starting Run: Batch Size 64 ===\n",
      "[RESUME] Found checkpoint for Batch 64. Loading...\n",
      "/home/padala.r/MyProject/single_gpu_amp_benchmark.py:151: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path, map_location=device)\n",
      "/opt/miniconda/envs/eai/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:822: UserWarning: When using ``NO_SHARD`` for ``ShardingStrategy``, full_state_dict willbe returned.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/eai/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:859: UserWarning: When using ``NO_SHARD`` for ``ShardingStrategy``, full_state_dict willbe returned.\n",
      "  warnings.warn(\n",
      "[RESUME] Resuming from Epoch 10\n",
      "\n",
      "=== Starting Run: Batch Size 128 ===\n",
      "[RESUME] Found checkpoint for Batch 128. Loading...\n",
      "/home/padala.r/MyProject/single_gpu_amp_benchmark.py:151: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path, map_location=device)\n",
      "/opt/miniconda/envs/eai/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:822: UserWarning: When using ``NO_SHARD`` for ``ShardingStrategy``, full_state_dict willbe returned.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/eai/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:859: UserWarning: When using ``NO_SHARD`` for ``ShardingStrategy``, full_state_dict willbe returned.\n",
      "  warnings.warn(\n",
      "[RESUME] Resuming from Epoch 10\n",
      "\n",
      "=== Starting Run: Batch Size 512 ===\n",
      "[RESUME] Found checkpoint for Batch 512. Loading...\n",
      "/home/padala.r/MyProject/single_gpu_amp_benchmark.py:151: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(ckpt_path, map_location=device)\n",
      "/opt/miniconda/envs/eai/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:822: UserWarning: When using ``NO_SHARD`` for ``ShardingStrategy``, full_state_dict willbe returned.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/eai/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:859: UserWarning: When using ``NO_SHARD`` for ``ShardingStrategy``, full_state_dict willbe returned.\n",
      "  warnings.warn(\n",
      "[RESUME] Resuming from Epoch 2\n",
      "Epoch 3/10 | Batch 512 | TrainAcc=0.8643 ValAcc=0.8781 | Time=555.71s Mem=1.86GB\n",
      "/opt/miniconda/envs/eai/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:768: UserWarning: When using ``NO_SHARD`` for ``ShardingStrategy``, full_state_dict willbe returned.\n",
      "  warnings.warn(\n",
      "/opt/miniconda/envs/eai/lib/python3.10/site-packages/torch/distributed/fsdp/_state_dict_utils.py:711: UserWarning: When using ``NO_SHARD`` for ``ShardingStrategy``, full_state_dict willbe returned.\n",
      "  warnings.warn(\n",
      "Epoch 4/10 | Batch 512 | TrainAcc=0.8749 ValAcc=0.8552 | Time=278.91s Mem=1.86GB\n",
      "Epoch 5/10 | Batch 512 | TrainAcc=0.8835 ValAcc=0.8498 | Time=271.97s Mem=1.86GB\n",
      "Epoch 6/10 | Batch 512 | TrainAcc=0.8905 ValAcc=0.8790 | Time=270.91s Mem=1.86GB\n",
      "Epoch 7/10 | Batch 512 | TrainAcc=0.8978 ValAcc=0.8903 | Time=268.34s Mem=1.86GB\n",
      "Epoch 8/10 | Batch 512 | TrainAcc=0.9021 ValAcc=0.8738 | Time=271.17s Mem=1.86GB\n",
      "Epoch 9/10 | Batch 512 | TrainAcc=0.9054 ValAcc=0.9057 | Time=259.51s Mem=1.86GB\n",
      "Epoch 10/10 | Batch 512 | TrainAcc=0.9088 ValAcc=0.9065 | Time=267.71s Mem=1.86GB\n",
      "\n",
      "All runs completed. Check FSDP_AMP_metrics.csv for results.\n"
     ]
    }
   ],
   "source": [
    "!python single_gpu_amp_benchmark.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65129c40-e286-4589-9e5f-524ddcc1db2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (hpc)",
   "language": "python",
   "name": "hpc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
